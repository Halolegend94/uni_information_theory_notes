\chapter{Introduction}
Information Theory is born from one man, Cloude Shannon (1926 - 2001). It has to do with probability, algebra, coding theory, ergodic theory and appears in daily life. 

\begin{itemize}
	\item Tom Cover, Joy Thomas, Information Theory, Wiley;
	\item IT: cooling theorems for discrete memoryless systems, Korner;
	\item IT, Robert Ash.
\end{itemize}

%In communication you want to be understood, but you also want to avoid redundancy and words tend to be lengthy; there is a conflict of interest. Why some words are short and others are long? Short words encode common abstract pictures whereas long words must carry the whole information not available in common knowledge. Suggested readings: 

%\section{Base concepts and considerations}
%Define $\msgset$ as the finite set of all messages over an alphabet $\Sigma$. A message $m$ is an element of $\msgset$. Hartley, who introduced this concept before Shannon, stated that the maximum amount of information is equal to $\log_2 |\msgset|$, since one can identify elements of $\msgset$ with binary strings of length $\log_2 |\msgset|$. Formally $$\msgset \sim \{0, 1\}^{\left\lceil\log_2 |\msgset|\right\rceil} $$ and $$ m \sim \Delta \in \{0, 1\}^{\left\lceil\log_2 |\msgset|\right\rceil}.$$

%One can think of $\Delta$ as a sequence of $\log_2 |\msgset|$ binary questions that identify an element in $\msgset$. A single unit of information is a $0$ or $1$ (Binary Information Theory). In the \emph{20 question game}, where you need to minimize the number of questions to guess what a person thinks, is given a probability distribution $P$ over $\msgset$ (denoted from now on as $P|M$) s.t. $P(m) \geq 0,\ \forall m \in \msgset$ and $\sum_{m \in \msgset} P(m) = 1$. The questions can be designed to make the game as short as possible if $P$ is known.

%Must be noted that $|\underline{x}| = i \Leftrightarrow \underline{x} \in \{0, 1\}^{i}$. The number of questions asked on average must be minimized. Let $f: \msgset \rightarrow \{0, 1\}^*$, in other words a mapping from the message set to the sequences of answers to find the message then we want to minimize 
%\begin{equation} \label{eq:problen}
%\sum_{m \in \msgset} P(m) |f(m)|.
%\end{equation} 
%This equation is tied only to the probability distribution, not to the set or the questions. If $P$ is uniform then the Expression \ref{eq:problen} is equal to $\left\lceil\log_2 |\msgset|\right\rceil$. The function $f$ is called \emph{code}. The code is not always invertible.

%Communication happens at a distance, otherwise it would be trivial. There is some device that makes communication possible; the channel is not perfect. The noise is random, modeled through a probability distribution. The information source is not predictable either and will have its own probability distribution too. Encoding and decoding must be optimized to make communications short to reduce loss of information, and to reliably transmit, i.e. the receiver is reasonably sure that it gets what was sent. The two aspects are in contrast, you need and optimal trade-off. Usually partners in communication are separated in space and communication takes place in time. Sometimes in computer science the opposite thing happens: communication is storing information in space to retrieve it later in time. Sometimes you want to shorten the communication time, sometimes the space communication takes.

\section{Lesson 2}
Define $\msgset$ as the finite set of all messages over an alphabet $\Sigma$. A message $m$ is an element of $\msgset$. Hartley, who introduced this concept before Shannon, stated that the maximum amount of information is equal to $\log_2 |\msgset|$, since one can identify elements of $\msgset$ with binary strings of length $\log_2 |\msgset|$. Formally $$\msgset \sim \{0, 1\}^{\lc\log_2 |\msgset|\rc} $$ and $$ m \sim \Delta \in \{0, 1\}^{\lc\log_2 |\msgset|\rc}.$$

One can think of $\Delta$ as a sequence of $\log_2 |\msgset|$ binary questions that identify an element in $\msgset$; a single unit of information is a $0$ or $1$ (Binary Information Theory). In the \emph{20 question game}, where you need to minimize the number of questions to guess what a person thinks, is given a probability distribution $P$ over $\msgset$ s.t. $P(m) \geq 0,\ \forall m \in \msgset$ and $\sum_{m \in \msgset} P(m) = 1$. The questions can be designed to make the game as short as possible if $P$ is known.

Must be noted that $|\underline{x}| = i \Leftrightarrow \underline{x} \in \{0, 1\}^{i}$. The number of questions asked on average must be minimized. Let $f: \msgset \rightarrow \{0, 1\}^*$, in other words a mapping from the message set to the sequences of answers to find the message then we want to minimize 
\begin{equation} \label{eq:problen}
\sum_{m \in \msgset} P(m) |f(m)|.
\end{equation} 
This equation is tied only to the probability distribution, not to the set or the questions. If $P$ is uniform then \ref{eq:problen} is equal to $\lc\log_2 |\msgset|\rc$. The function $f$ is called \textbf{code}. The code is not always invertible.

Communication happens at a distance, otherwise it would be trivial. There is some device that makes communication possible; the channel is not perfect. The noise no it is random, modelled through a probability distribution. The information source is not predictable either and will have itsoen probability distribution too ['nche senso, di errore o tempo di trasmissione?]. Encoding and decoding must be optimized to make communications short to reduce ``lost'', and to reliably transmit, i.e. the receiver is reasonably sure that it gets what was sent. The two aspects are in contrast, you need and optimal trade-off. Usually partners in communication are separated in space and communication takes place in time. Sometimes in computer science the opposite thing happens: communication is storing information in space to retrieve it later in time. Sometimes you want to shorten the communication time, sometimes the space communication takes. The model is the same. Entropy comes from thermodinamics, introduced by Boltzmann.
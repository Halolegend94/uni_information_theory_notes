
\chapter{The communication model}

Shannon developed a mathematical model of the communication among parties.
In particular, we will consider the simplest one, in which the communication is one-way between two entities.
In this model, one entity is the \emph{source} that sends information to the second entity, called the \emph{receiver}, through a \emph{noisy channel}.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{pictures/comm-channel.eps}
	\caption{Graphic model of a simple communication.}
\end{figure}

Source and receiver are usually separated in space and communicate (approximately) at the same time.
Indeed, the roles of space and time can be swapped, and we could think of a communication that evolves over time at the same place, like data storage and retrieval.

The noisy channel is both the vehicle and the obstacle.
One cannot transmit a signal without it being modified to some extent; moreover the communication must be quick because time is expensive.
The alterations to the message are errors and therefore must be corrected.
In order to do this we need to remove ``bad'' redundancy and add ``good'' redundancy.
So we have a trade-off between data integrity and fast communication.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{pictures/comm-channel1.eps}
	\caption{The role of encoder and decoder.}
\end{figure}

The balance is found by the \emph{encoder}.
This entity takes the message from the source, modify it in some way, and transmit the result over the channel.
A \emph{decoder} is the entity that apply some transformation to the incoming flow of information before handling it to the receiver.
The decoder cannot perform the inverse of the encoder's function, because the channel applies noise to the information.
We will concentrate on the noisy channel, without considering encoder and decoder's interfaces to source and receiver.

%NOTE: here he said something on asimptotic equipartition

\section{\acl{DMC}}

For now, the channel is a black box.
Let $\X$ be the input alphabet and $\Y$ be the output alphabet for the strings that respectively enter and exit the channel.
Select an input symbol $x \in \X$; a random symbol $y \in \Y$ comes out of the channel.

We can think that there is a ``devil'' inside the black box.
The devil has dices that have $\abs{\Y}$ faces.
Whenever an input signal enters the channel, the devil launches a dice and outputs the symbol on the facet that comes up.
The devil cannot launch any dice he likes; instead, there is a mapping between dices and input symbols (you can think of $x$ as the name of the dice that the devil has to use).
So we can influence the devil in using some specific dice (out of the $\abs{\X}$ dices), but the outcome is still stochastic: the probability distribution of each dice is arbitrary.
Note that the channel is usable if we have different dices; otherwise, the output is completely uncontrollable.

Let $W(y|x)$ be the probability that the dice named $x$ will fall onto its facet $y$, with the following conditions: 
\begin{itemize}
	\item $W(y|x) \ge 0$,
	\item $\forall x . \sum_{y \in \Y} W(y|x) = 1$.
\end{itemize}
We can think of $W$ as a matrix whose columns are indexed by $y$ and rows are indexed by $x$.

To communicate iteratively pick $x_1, x_2, \dots, x_n \in \X^n$ and launch a sequence of dices.
There is a probability distribution for $y_1, y_2, \dots, y_n \in Y^n$. Consider the function
\begin{equation*}
	W^n : \X^n \to \Y^n,
\end{equation*}
defined as
\begin{equation*}
	W^n \left( \str{x}, \str{y} \right)
	= \prod_{i = 1}^n W \left( y_i | x_i \right).
\end{equation*}

We assume that every symbol is independently modified (lack of memory of the channel).
Our \ac{DMC} (over time instants) is defined by the set $\left\{ W^n \right\}_{n = 1}^{\infty}$.

\section{Shannon's Noisy Channel Theorem}

The encoder takes the messages of the source and maps it to an arbitrary subset $\C \subseteq \X^n$.
So we have $\abs{\C}$ many messages that can flow in the channel; they can be non-binary strings, but to have an approximation of their length we can assume they are binary.
Then, the length of these messages is $\logtwo{\abs{\C}}$.
We define the speed of the channel as
\begin{equation*}
	\frac{1}{n} \cdot \logtwo{\abs{\C}},
\end{equation*}
where $n$ is the length of transmission in some unit of time.
We can call this \emph{rate} of the code.

Now we are interested in the quality of transmission, that is how well data transmitted can be recovered.
The decoder is a function $\phi: \Y^n \to \C$.
An error event happens when $\str{x} \in \X^n$ is transmitted and $\phi(\str{y}) \neq \str{x}$ is received.
Define
\begin{equation*}
	W^n \left( T|\str{x} \right)
	=
	\sum_{\str{y} \in T} W^n \left( \str{y}|\str{x} \right),
\end{equation*}
where $T \subseteq \Y$.
The error probability of a string $\str{x} \in \X^n$ is
\begin{equation*}
	1 - W^n \left( \phi^{-1}(\str{x}) | \str{x} \right)
	=
	W^n \left( \overline{\phi^{-1}(\str{x})} | \str{x} \right),
\end{equation*}
where
\begin{equation*}
	\phi^{-1}(\str{x}) = \left\{ \str{y} : \phi(\str{y}) = \str{x} \right\}.
\end{equation*}

Define the (maximum) error probability of the code $(\C_n, \phi_n)$ ($W^n$ is fixed) as 
\begin{equation*}
	e_m \left( W^n, \C_n, \phi_n \right)
	=
	\max_{\str{x} \in \C} W^n \left( \overline{\phi^{-1}(\str{x})}, \str{x} \right).
\end{equation*}

\begin{prop}
	$R \geq 0$ is an \emph{achievable rate} of transmission over the \ac{DMC} $\{ W \}$ if exists $\{(\C_n, \phi_n)\}_{n=1}^\infty$ such that:
	\begin{itemize}
		\item $\lim_{n \to \infty} \frac{1}{n} \logtwo{\abs{\C_n}} \ge R$;
		\item $\lim_{n \to \infty} e_m(W, \C_n, \phi_n) = 0$.
	\end{itemize}
\end{prop}

What is the highest achievable rate?
\begin{equation*}
	\C \subseteq \X^n \implies \frac{1}{n} \logtwo{\abs{C}} \le \logtwo{\abs{\X^n}}.
\end{equation*}
So the maximum achievable rate is $\logtwo{\abs{\X^n}}$.
Also, if $R_t$ is a series of achievable rates and $R_t$ tends to $R$, then $R$ is an achievable rate too (simple analysis, will not be demonstrated).

Shannon wondered about the highest achievable rate $C(W)$ of a given DMC $\{W\}$.
He called this number \emph{capacity of the channel}.
The answer came out of his intuition but it has been proved true by his students and co-workers.

Let $x \in \X$ be a symbol chosen following a probability distribution $P|\X$.
The input symbol will be sent through the channel $W$ and the output is conditioned by $W$.
Let $Y$ be a \ac{RV} $Y \in \Y$ representing the output of $W$ with respect to input $X$.
Then, we have a joint distribution $P, W | \X \times \Y$.
The probability that a specific input $x$ corresponds to a received symbol $y$ is
\begin{equation*}
	\Pr{X = x, Y = y} = P(x) W(y|x).
\end{equation*}

Define $\MutualInformation{X \land Y}$ to be the number of bits that one can transmit over a channel in a unit of time.
Intuitively, it is what $y$ retains of input $x$ (mutual information).
We can maximize $I$ by controlling the probability distribution $P|\X$.
With some abuse of notation:
\begin{equation*}
	\max_{P|\X} \MutualInformation{P, W}.
\end{equation*}
What is the formula for $I$?
\begin{align*}
	\MutualInformation{X \land Y}
	& =
	\sum_{(x, y) \in \X \times \Y}
	\Pr{X=x, Y=y} \cdot
	\logtwo{\frac{\Pr{X=x, Y=y}}{\Pr{X=x} \cdot \Pr{Y=y}}}
	\\
	& =
	\sum_{x \in \X,  y \in \Y}
	\Pr{X=x} \cdot W(y|x) \cdot
	\logtwo{\frac{\Pr{X=x} \cdot W(y|x)}{\sum_{x \in X} \Pr{X=x} \cdot W(y|x)}}
	\\
	& =
	\MutualInformation{P, W}.
\end{align*}

\begin{thm}[Shannon's Noisy Channel Theorem]\label{thm:nct}
	The highest achievable rate, given a \ac{DMC} $W$, is
	\begin{equation*}
		C(w) = \max_{P|X} \MutualInformation{P, W}.
	\end{equation*}
\end{thm}

Notice that $C(W)$ is 0 if $X$ and $Y$ are independent, \ie all rows in $W$ are the same. 

\section{\acl{BSC}}

A \ac{BSC} is used to transmit binary strings and the \ac{DMC} has the form
\begin{equation*}
	W = 
	\begin{pmatrix}
		1-p & p \\
		p & 1-p
	\end{pmatrix},
\end{equation*}
with $\X = \Y = \{0,1\}$.

It is called symmetric because $W$ is symmetric.
In this context $P|\X$ is called \emph{crossover} probability.
The capacity of this channel according to \cref{thm:nct} is
\begin{equation*}
	\MutualInformation{X \wedge Y}
	=
	\Entropy{Y} - \Entropy{Y|X}
	\le
	1 - \entropy{p}.
\end{equation*}

This is true because:
\begin{itemize}
	\item the maximum value for $\Entropy{Y}$ is 1;
	\item the following equations hold:
		\begin{equation*}
			\Entropy{Y | X} =
			\sum_{x \in \X} \Pr{X = x} \cdot \Entropy{Y | X = x} =
			\entropy{p},
		\end{equation*}
		where
		\begin{equation*}
			\Entropy{Y | X = x} =
			\sum_{y \in \Y} \Pr{Y = y | X = x} \cdot
			\logtwo{\frac{1}{\Pr{Y = y | X = x}}}.
		\end{equation*}
\end{itemize} 

Is this upper bound achievable? We must make sure that
\begin{equation*}
	\exists X : \Entropy{Y} = 1.
\end{equation*}
We can impose the uniform distribution when picking the input values, \ie $\Pr{X = 0} = \frac{1}{2}$.

\begin{obs}
	$1-\entropy{2p}$ is an achievable rate if $p < \frac{1}{4}$.
\end{obs}
We will do this using error correcting codes.

\begin{proof}
	Choose arbitrary $\str{x} \in \{0, 1\}^n$, but think of $\zero$ (it's easier).
	Consider the Hamming ball $\hball{\zero}{n(p+\epsilon)}$; we want that
	\begin{equation*}
		W^n \left( \phi^{-1}_n(\zero) | \zero \right) \to 1.
	\end{equation*}
	Consider a code for which
	\begin{equation*}
		\forall \str{x} \in \C_n .
		\phi^{-1}_n(\str{x}) \supseteq \hball{\str{x}}{n(p+\epsilon)};
	\end{equation*}
	these Hamming balls should be disjoint.
	We wonder if it is true that
	\begin{equation*}
		W^n \left( \hball{\zero}{n(p+\epsilon)} | \zero \right) \to 1.
	\end{equation*}
	This is what we should achieve, and prove that $np$ is a good choice.
	What we receive is $\str{x} \xor Z^n$, for some \ac{RV} $Z^n$, defined as $Z^n = \str{x} \xor Y^n$.
	\begin{equation*}
		Y^n \in \hball{\zero}{n(p+\epsilon)} \iff Z^n \text{ has at most } n(p+\epsilon) \text{ 1s}. 
	\end{equation*}

	The number of 1s in $Z^n$ is equal to $\sum_{i = 1}^{n} Z_i$, where $Z_i \sim (1-p, p)$ is a \ac{RV}.

	\begin{equation*}
		\expectation{Z_i} = \Pr{Z_i = 0} \cdot 0 + \Pr{Z_i = 1} \cdot 1 = p.
	\end{equation*}

	$\{Z_i\}_{i=1}^n$ is an \ac{IID} sequence of \acp{RV}.
	\begin{equation*}
		\expectation{Z^n} =
		\expectation{\sum_{i = 1}^{n} Z_i} =
		\sum_{i = 1}^{n} \expectation{Z_i} =
		n \cdot p.
	\end{equation*}

	By the law of large numbers,
	\begin{equation*}
		\Pr{\abs{\frac{1}{n} \sum_{i = 1}^{n} Z_i - p} > \epsilon} \to 0.
	\end{equation*}
	The complement of this event has a probability that converges to 1.
	How can we guarantee that these Hamming balls of radius $n(p+\epsilon)$ are disjoint?
	This is guaranteed if $\hdist{\str{x}',\str{x}''} \ge 2n (p+\epsilon)$.
	Thus $\hdist{\C_n} \ge 2n(p+\epsilon)$.
	The Gilbert-Varhsamov bound (\cref{thm:gilbert-varshamov}) says that, if $\delta < \frac{1}{2}$,
	\begin{equation*}
		\exists \C_n : \abs{\C_n} \gtrsim 2^{n \, (1-\entropy{\delta})},
	\end{equation*}
	and that
	\begin{equation*}
		\hdist{\C_n} \gtrsim n \delta.
	\end{equation*}

	We want that $d_H(\C_n) \ge 2n(p+\epsilon)$, thus what we want is achievable if and only if
	\begin{equation*}
		2p + \epsilon < \dfrac{1}{2} \sim p < \dfrac{1}{4}. \qedhere
	\end{equation*}
\end{proof}

Shannon stated something stronger, namely that the capacity is $1 - \entropy{p}$.
Shannon uses ``maximum likelihood''.
He chooses $2^{n \, R}$ strings at random from $T^n_p$, given $P|\X$.

\begin{thm}[Converse part of Shannon's noisy channel coding theorem]
	If $R$ is such that $\exists \{\C_n, \phi_n\}_{n=1}^\infty$ with
	\begin{equation*}
		\limsup_{n \to \infty}
		\frac{1}{n} \cdot \logtwo{\abs{\C_n}} \ge R,
	\end{equation*}
	and
	\begin{equation*}
		\lim_{n \to \infty} e_n (W^n, \C_n, \phi) = 0,
	\end{equation*}
	then
	\begin{equation*}
		R \le \max_{
			\substack{
				X \in \X : \\
				P_{Y|X} = W
			}
		}
		\MutualInformation{X \land Y}.
	\end{equation*}
\end{thm}

\begin{proof}
	Let $M_n$ be a \ac{RV} uniformly distributed over $\C_n$.
	Take $\frac{1}{n} \cdot \Entropy{M_n}$, the average entropy.
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n} =
		\frac{1}{n} \cdot \logtwo{\abs{\C_n}}.
	\end{equation*}
	To get the upper bound, we manipulate the \ac{LHS}.
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n} =
		\frac{1}{n} \cdot \left[ \Entropy{M_n} - \Entropy{M_n | \phi_n(Y^n)} \right] +
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y^n)},
	\end{equation*}
	where $Y^n$ is the \ac{RV} defined by $P_{Y^n | M_n} = W^n$.
	In the square brackets we have a mutual information, so
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n} =
		\frac{1}{n} \cdot \MutualInformation{M_n \land \phi_n(Y^n)} +
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y^n)}.
	\end{equation*}
	We should be able to prove that this is not more than the upper bound.
	We expect the other therm to be negligible; with high probability the result of decoding the received codeword is the codeword that was sent.
	\begin{equation*}
		\MutualInformation{M_n \land \phi_n(Y^n)} \le
		\MutualInformation{M_n \land Y^n}.
	\end{equation*}
	This is from the fact that information cannot be gained by processing the \acp{RV}.
	We see this by looking at
	\begin{equation} \label{eq:dis1}
		\MutualInformation{M_n \land \phi_n(Y^n)} \le
		\MutualInformation{M_n \wedge \phi_n(Y^n), Y^n},
	\end{equation}
	since
	\begin{equation*}
		\Entropy{M_n} - \Entropy{M_n | \phi_n(Y^n)} \le
		\Entropy{M_n} - \Entropy{M_n | \phi_n(Y^n), Y^n},
	\end{equation*}
	and since
	\begin{equation*}
		\Entropy{M_n | \phi_n(Y^n)} \ge
		\Entropy{M_n | \phi_n(Y^n), Y^n},
	\end{equation*}
	\Cref{eq:dis1} follows.

	The \ac{RHS} of \cref{eq:dis1} can be written as
	\begin{align*}
		\MutualInformation{M_n \land \phi_n(Y^n), Y^n}
		& =
		\MutualInformation{M_n \land Y^n} +
		\MutualInformation{M_n \land \phi_n(Y^n) | Y^n}
		\\
		& =
		\MutualInformation{M_n \land Y^n}.
	\end{align*}
	we see that
	\begin{equation*}
		\MutualInformation{M_n \land \phi_n(Y^n) | Y^n} \le
		\Entropy{\phi_n(Y^n) | Y^n} = 0.
	\end{equation*}

	So we obtain the following inequality:
	\begin{equation*}
		\MutualInformation{M_n \land \phi_n(Y^n)} \le
		\MutualInformation{M_n \land Y^n}.
	\end{equation*}
	As notation, $M_n = X_1 \dots X_n$ is a vector of \acp{RV}.
	Thus,
	\begin{equation*}
		\MutualInformation{M_n \land Y^n} =
		\MutualInformation{X^n \land Y^n}.
	\end{equation*}
	This is what we want to upper bound with the conjectured value of capacity.
	We write
	\begin{equation*}
		\MutualInformation{X^n \land Y^n} =
		\Entropy{Y^n} - \Entropy{X^n | Y^n} \le
		\sum_{i = 1}^n \Entropy{Y_i} - \Entropy{Y^n | X^n}.
	\end{equation*}
	For $\Entropy{Y^n | X^n}$ we use the chain rule:
	\begin{equation*}
		\Entropy{Y^n | X^n} =
		\sum_{i = 1}^n \Entropy{Y_i | X^n, Y_1, \dots, Y_{i-1}}.
	\end{equation*}

	But in a \ac{DMC} we have that
	\begin{equation*}
		\Entropy{Y_i | X^n, Y_1, \dots, Y_{i-1}} =
		\Entropy{Y_i | X_i},
	\end{equation*}
	so
	\begin{equation*}
		\Entropy{Y^n | X^n} = \sum_{i = 1}^{n} \Entropy{Y_i | X_i}.
	\end{equation*}

	So what we get is that
	\begin{equation*}
		\MutualInformation{X^n \land Y^n} =
		\sum_{i = 1}^{n} \MutualInformation{X_i \land Y_i}.
	\end{equation*}
	This is true because our channel is a particular type of channel.

	\begin{equation*}
		\sum_{i = 1}^{n} \MutualInformation{X_i \land Y_i} \le
		n \cdot \max_{P_{Y|X} = W} \MutualInformation{X \land Y}.
	\end{equation*}
	which is $n$ times capacity.
	So:
	\begin{equation*}
		\frac{1}{n} \cdot \MutualInformation{M_n \land \phi_n(Y^n)} \le C(W).
	\end{equation*}

	For the second part, we have to prove that 
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y^n)} \to 0.
	\end{equation*}

	We do so using Fano's inequality.
	We introduce the \ac{RV} $Z_n$ defined as
	\begin{equation*}
		Z_n =
			\begin{cases}
				1 \text{ if } \phi_n(Y^n) \neq M_n,\\
				0 \text{ otherwise.}
			\end{cases} 
	\end{equation*}

	We can say that
	\begin{equation*}
		\Entropy{M_n | \phi_n(Y^n)} \le
		\Entropy{M_n, Z_n | \phi_n(Y^n)}
	\end{equation*}
	since we ``added'' something to a \ac{RV}.

	\begin{align*}
		\Entropy{M_n, Z_n | \phi_n(Y^n)}
		& =
		\Entropy{Z_n | \phi_n(Y^n)} +
		\Entropy{M_n | \phi_n(Y^n), Z_n}
		\\
		& \le
		\Entropy{Z_n} +
		\Entropy{M_n | \phi_n(Y^n), Z_n}
		\\
		& \le
		1 +
		\Entropy{M_n | \phi_n(Y^n), Z_n}.
	\end{align*}
	So for now we have that
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y^n)} \le
		\frac{1}{n} + \frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y_n), Z_n}.
	\end{equation*}
	We have to show that
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y_n), Z_n}
	\end{equation*}
	is small.
	We can break this up:
	\begin{equation} \label{eq:eqbreak}
	\begin{aligned}
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y_n), Z_n}
		= &~
		\Pr{Z_n = 1} \cdot \Entropy{M_n | \phi_n(Y^n), Z_n = 1} +
		\\
		&~+
		\Pr{Z_n = 0} \cdot \Entropy{M_n | \phi_n(Y^n), Z_n = 0}.  
	\end{aligned}
	\end{equation}
	Now we have to bring in the error probability.
	\begin{align*}
	  \epsilon_n
	  & =
	  \Pr{Z_n = 1} =
	  \Pr{M_n \neq \phi_n(Y^n)}
	  \\
	  & \le
	  e_n(W^n, \C_n, \phi_n) \to 0.
	\end{align*}
	So $\epsilon_n \to 0$, since the average is not more than the maximum.
	\begin{align*}
		\Entropy{M_n | \phi_n(Y^n), Z_n = 1}
		& =
		\\
		\sum_{y \in \Y^n} \Pr{Y^n = y}
		& \cdot
		\Entropy{M_n | \phi_n(Y^n) = \phi_n(y), Z_n = 1}.
	\end{align*}

	$M_n \in \X^n$.
	So that entropy is less than $\logtwo{\abs{\X^n}}$.
	So the first term of \cref{eq:eqbreak} can be bounded by
	\begin{equation*}
		\epsilon_n \cdot
		\frac{1}{n} \cdot
		\Entropy{M_n | \phi_n(Y^n), Z_n = 1} \le
		\epsilon_n \cdot
		\frac{1}{n} \cdot
		\logtwo{\abs{\X^n}} =
		\epsilon_n \cdot \logtwo{\abs{\X}} \to 0.
	\end{equation*}
	The second term of \cref{eq:eqbreak} is equal to 0.
	\begin{equation*}
		\frac{1}{n} \cdot \Pr{Z_n = 0} \cdot \Entropy{M_n | \phi_n(Y^n), Z_n = 0}.
	\end{equation*}
	To see why, note that there is no error, so $M_n = \phi_n(Y^n)$.
\end{proof}


\section{Zero Error Capacity}
Zero stands for zero probability, so the probability of error is bound to be equal to 0. We translate this problem in graph theory: we will look
at product of graphs and powers of graphs. We have a DMC $\{W\},\ W:\X \rightarrow \Y$. $W^n:\X^n \rightarrow \Y^n$, the mathematical model of a code is the very same. A code is $(\C_n, \phi)$ with $\C_n \subseteq \X^n$ and $\phi: \Y^n \rightarrow \C_n$. A code has two parameters:

\begin{itemize}
	\item $\dfrac{1}{n}|\C_n|$ is the size of the code compared to $n$. It is the number of bits transmitted over the channel per use of the channel.
	\item $e_n(W^n, \C_n, \phi) = \max_{\str{x} \in \C_n} W^n(\overline{\phi^{-1}(\str{x})}|\str{x})$ is the probability of error.
\end{itemize}

$(\C_n, \phi)$ is a zero error code if $e_n(W^n, \C_n, \phi)=0$. $R \geq 0$ is an achievable rate for error probability $0$ if $\exists (\C_n, \phi_n)$, $\C_n \subseteq \X^n$ with
\[
\overline{\lim_{n \rightarrow \infty}} \dfrac{1}{n}\log|\C_n| \geq R\text{ and }
\]

\[
e_n(W^n, \C_n, \phi_n) = 0.
\]

If $W(y|x) > 0$ for all $x, y$ then this is not possible. If you have 0s in the matrix, we are only interested in the patterns of the 0s. To have 0 error, we should have that
\[
W^n(\phi^{-1}(\str{x})|\str{x}) = 1,\ \forall\str{x} \in \C_n
\]

The decoding function must be fixed if we want $0$ error. If $W^n(\str{y}|\str{x}) \geq 0$, we must have that $\phi^{-1}(\str{y}) = \str{x}$. Given $\str{x}$, the set $\{\str{y}| W^(\str{y}|\str{x})>0 \}$ is uniquely defined. This is the support of conditional distribution:
\begin{equation}
\supp(\str{x}) = \{\str{y}| W^n(\str{y}|\str{x})>0 \}
\end{equation}

We should have that 
\[
\supp(\str{x}) \subseteq \phi^{-1}(\str{x}),\ \forall\str{x} \in \C_n
\]

So this,\ie how to get the 0 error probability, only depends on the set $\C_n$. Support sets have to be disjoint. $\C_n$ is a $0$ error code if
\[
\forall \{\str{x}',\str{x}'' \} \in \binom{\C_n}{2}\ \supp(\str{x}') \cap \supp(\str{x}'') = \emptyset
\]

\begin{obs}
	Let $\str{x} \in \X^n$. Then $$\str{y} \in \supp(\str{x}) \Leftrightarrow W^n(\str{y}|\str{x}) >0,$$ but $W^n$ is a product of probabilities; thus, since
	$$W^n(\str{y}|\str{x}) = \prod_{i=1}^nW(y_i|x_i)$$
	it must be that
	$$W(y_i, x_i) > 0,\ \forall i.$$ So the support set is $$\supp(\str{x}) = \times_{i=1}^n\supp(x_i)$$
\end{obs}

This is a combinatorial condition. The support is the set of positive elements of the row of $x$. You need at least two orthogonal rows in the matrix.

\begin{obs}
	$$\supp(\str{x}') \cap \supp(\str{x}'') = \emptyset \Leftrightarrow \supp(x_i') \cap \supp(x_i'') = \emptyset,$$ for some $i$. 
\end{obs}
\noindent\textbf{Proof}. Suppose 
\begin{align*}
	&\supp(\str{x}') \cap \supp(\str{x}'') \not= \emptyset\\
	&\Leftrightarrow \exists\str{z} \in \supp(\str{x}') \cap \supp(\str{x}'')\\
	& \Leftrightarrow z_i \in \supp(x') \cap \supp(x''),\ \forall i
\end{align*}
$\hfill\Box$

Consider the graph $G = G(W)$ such that $V(G) = \X$ and $\{x',x'' \} \in E(G)$ if $\supp(x') \cap \supp(x'') = \emptyset$. We are looking for a clique in this graph, which is the size of the best code one can have $\omega(G)$ (?) What about when we use the channel more than once? We look at $G^n = G(W^n)$, where $V(G^n) = \X^n$ and $\{\str{x}',\str{x}''\} \in E(G^n)$ if $\exists i\ \supp(x_i') \cap \supp(x_i'') = \emptyset$. This sequence of graphs can be built using the first graph, not using always the matrix.

\noindent From $G$ to $G^n$.

\noindent Take $G$. If $\{a, b\} \in E(G)$ they are indistinguishable. Now take $G^n$. $\{\str{a}, \str{b}\} \in E(G^n)$ are indistinguishable if $\exists i \{a_i, b_i\} \in E(G)$ with $a = a_1\ldots a_n$ and $b=b_1\ldots b_n$. Every clique of maximal length is a 0 error code.

We say that 
\[
\overline{\lim_{n \rightarrow \infty}}\dfrac{1}{n}\log \omega(G^n)
\]

is the zero error capacity of $G$. This limit exists for every graph.

\begin{prop}
	For every graph $G\ \exists W\ s.t.\ G = G(W)$.
\end{prop}

Consider a matrix $W$ where the rows are the vertexes of the graphs and columns are indexed by the non-edges, \ie the pairs of $\binom{V(G)}{2}$ that are not in $E(G)$.

\begin{equation}
	W(\{a,b\}|x) = \begin{cases}
	0\ if\ x\not\in \{a, b\}\\
	1\ if\ x\in \{a, b\}
	\end{cases}
\end{equation}

If the rows of $x$ and $y$ are not orthogonal, $\{x, y\}$ is not an edge. There are two problems:
\begin{itemize}
	\item rows could not sum to 1;
	\item rows could sum up t 0.
\end{itemize}

The first problem is avoided by dividing the rows. The second is a avoided by doing
\[
W = (W\ I)
\]

then $W$ can be normalized. Now $G = G(W)$.

$\hfill\Box$

$ \sqrt[n]{\omega(G^n)}$ is how much larger is $\omega(G^n)$ with respect to $\omega(G)$. For graphs of $5$ vertices there is only one graph for which
\[
 \sqrt[n]{\omega(G^n)} \not=\omega(G),
\]
it is the following.
\begin{figure}[h!]
	\centering
	\includegraphics[width=100px]{pictures/graph5v.eps}
\end{figure}

Lovàsz gave a function for $ \sqrt[n]{\omega(G^n)}$ in '79. In fact it works for graphs that are self complementary and vertex transitive.

\begin{equation}
	C(G) = \overline{\lim_{n\rightarrow \infty}}  \sqrt[n]{\omega(G^n)}
\end{equation}

Shannon proved that
\[
\sqrt{5} \leq C(C_5) \leq \dfrac{5}{2}
\]

Lovàsz proved that in fact $C(C_5) = \sqrt{5}$. For the upper bound we have that $C_5 = G(W)$ with
\[
 W = \left(\begin{array}{c c c c c}
 \frac{1}{2} &\frac{1}{2} & 0&0&0\\
 0&0&\frac{1}{2}&\frac{1}{2}&0\\
 \frac{1}{2}&0&0&0&\frac{1}{2}\\
 0&\frac{1}{2}&\frac{1}{2}&0&0\\
 0&0&0&\frac{1}{2}&\frac{1}{2}\\
 \end{array}\right)
\]

then said that 0 error capacity is less than or equal to the capacity. Shannon noticed, for the lower bound, that $C(G) > \omega(G)$, but $\omega(G) = 2$. If you look at $G^2$, \ie $C_5^2$, then $\omega(C_5^2) = 5$.
\[
\omega(G^n) \geq [\omega(G)]^n
\]
with the product set you get $4$. But Shannon noticed that those $5$ strings are distinguishable:
\[
\{00,11,24,31,43\}.
\]
\[
 \sqrt[n]{\omega(G^n)} \geq \omega(G)
\]
\[
 C(G) = \overline{\lim_{n\rightarrow \infty}}\sqrt[n]{\omega(G^n)}
\]
That is, Shannon capacity of G.

\begin{lem}[Fevete]
Take a sequence of reals $a_n \in \Reals$, if the sequence id super additive, $\ie\ a_{m+n} \geq a_m + a_n\ \forall m, n \in \mathbb{N}$ and
\[
 \dfrac{a_n}{n} \leq M\ \forall n \in \mathbb{N}
\]

then
\[
 \exists \lim_{n \rightarrow \infty}  \dfrac{a_n}{n} 
\]

and

\[
\lim_{n \rightarrow \infty}  \dfrac{a_n}{n} = \sup_n \dfrac{a_n}{n}. 
\]
 
\end{lem}

To apply this to $\omega(G^n)$ we have to take the logarithm, since $\omega(G^n)$ is super multiplicative (and thus $\log(\omega(G^n))$ is super additive). We have that $\omega(G^n) \leq |V(G)|^n$, and thus
\[
 \dfrac{\log(\omega(G^n))}{n} \leq \log|V(G)|
\]

since 

\[
 \dfrac{a_n}{n} \leq M,\ \exists \hat{M} = \sup_n \dfrac{a_n}{n}\ \text{with}\ \hat{M} \leq M,
\]

so
\[
 \forall \epsilon\ \exists n_0\ \dfrac{a_{n_0}}{n_0} > \hat{M} - \epsilon.  
\]

Take arbitrary $n > n_0$ with $n = q_nn_0 +r_n$, with $0 \leq _n < n_0$.

\begin{align*}
 \dfrac{a_n}{n} = \dfrac{a_{q_nn_0 +r_n}}{q_nn_0 +r_n} & \geq \dfrac{q_na_{n_0} +a_{r_n}}{(q_n+1)n_0}\\
 & = \dfrac{q_n}{q_{n+1}} \dfrac{a_{n_0}}{n_0} + \dfrac{a_{r_n}}{(q_n + 1)n_0}.
\end{align*}

so the limit can be bounded from below

\[
 \underline{\lim_{n\rightarrow \infty}} \dfrac{a_n}{n} \geq \underline{\lim_{n\rightarrow \infty}}\left(\dfrac{q_n}{q_{n+1}} \dfrac{a_{n_0}}{n_0} + \dfrac{a_{r_n}}{(q_n + 1)n_0}\right) \geq \hat{M} - \epsilon
\]

(since $a_{r_n} \geq \min\{a_0, \ldots,a_{n_0 -1}\}$ the last fraction goes to 0).

$\hfill\Box$

\begin{prop}
 $$\omega(G) \leq C(G) \leq \chrom{G},$$ where $\chrom{G}$ is the chromatic number of $G$.
\end{prop}

A colouring of $G$ is a function $f: V(G) \rightarrow C$ such that if $\{a, b\} \in E(G)$ then $f(a) \not= f(b)$. $\chrom{G}$ is the minimum cardinality of $C$ such that a function $f: V(G) \rightarrow C$ like that exists.

\noindent\textbf{Proof}. 
\[
 [\omega(G)]^n \leq \omega(G^n) \leq \chrom{G^n}
\]

the first part comes from super-multiplicativity of $\omega(G)$, the second from the fact that the chromatic number of a graph is greater than the largest clique. Since $\chrom{G}$ is sub-multiplicative, we have $$\chrom{G^n} \leq [\chrom{G}]^n.$$

To show that $\chrom{G}$ is sub-multiplicative, consider an optimal colouring of $G$. $$f:V(G) \rightarrow C\ \text{with}\ |C| = \chrom{G}$$ take $\str{x} \in V(G^n)$, we colour it separately, for $x_i$ in $x_1\ldots x_n$. The function $f^n(\str{x})$, the extension by concatenation, id a correct colouring. If $\{\str{x}, \str{y}\} \in E(G^n)$, it must be that $\exists i : \{x_i, y_i\} \in E(G)$, but then, since $f(x_i) \not=f(y_i)$, $f^n(\str{x}) \not= f^n(\str(y))$. Also, the number of colours used by $f^n$ id no more than $|C^n| = [\chrom{G}]^n$, so
\[
 \omega(G) \leq \sqrt[n]{\omega(G^n)} \leq \chrom{G}
\]

$\hfill\Box$

[Note: There is a simple upper bound that is better than this. You use the ``fractional'' chromatic number. You express colouring as PU (?) and drop integer constant.]

\begin{cor}
 $$\omega(G) = \chrom{G} \Rightarrow C(G) = \omega(G).$$
\end{cor}

The corollary doesn't give us much. You can make any graph like this. Schutzenberger + Berge showed some classes of graphs for which this holds. One example is graphs for which vertices are intervals, and intersecting intervals share and edge. Furthermore, $\forall G' \subseteq G$ induced $\omega(G') = \chrom(G')$, for interval graphs.

\begin{definition}[Perfect graph]
 $G$ i perfect if $$\omega(G') = \chrom{G'}\ \forall G' \subseteq G,$$ where $G'$ is an induced subgraph.
\end{definition}

What makes this definition beautiful are two conjectures, by Berge. By now they are both theorems.
\begin{itemize}
 \item \textbf{Weak}: $G$ is perfect $\Leftrightarrow \overline{G}$ is perfect.
 \item \textbf{Strong}: minimally imperfect graphs are either odd cycles or their complements.
\end{itemize}

\begin{definition}[Minimally imperfect graph]
 A graph $G$ is minimally imperfect if
 \begin{enumerate}
  \item $G$ is not perfect, \ie $\omega(G) < \chrom{G}$,
  \item $\forall G' \subseteq G$ induced subgraph, $G'$ is perfect, \ie $\omega(G') = \chrom{G'}$.
 \end{enumerate}

\end{definition}





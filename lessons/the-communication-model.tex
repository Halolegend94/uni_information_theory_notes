
\chapter{The communication model}

Shannon developed a mathematical model of the communication among parties.
In particular, we will consider the simplest one, in which the communication is one-way between two entities.
In this model, one entity is the \emph{source} that sends information to the second entity, called the \emph{receiver}, through a \emph{noisy channel}.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{pictures/comm-channel.eps}
	\caption{Graphic model of a simple communication.}
\end{figure}

Source and receiver are usually separated in space and communicate (approximately) at the same time.
Indeed, the roles of space and time can be swapped, and we could think of a communication that evolves over time at the same place, like data storage and retrieval.

The noisy channel is both the vehicle and the obstacle.
One cannot transmit a signal without it being modified to some extent; moreover the communication must be quick because time is expensive.
The alterations to the message are errors and therefore must be corrected.
In order to do this we need to remove ``bad'' redundancy and add ``good'' redundancy.
So we have a trade-off between data integrity and fast communication.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{pictures/comm-channel1.eps}
	\caption{The role of encoder and decoder.}
\end{figure}

The balance is found by the \emph{encoder}.
This entity takes the message from the source, modify it in some way, and transmit the result over the channel.
A \emph{decoder} is the entity that apply some transformation to the incoming flow of information before handling it to the receiver.
The decoder cannot perform the inverse of the encoder's function, because the channel applies noise to the information.
We will concentrate on the noisy channel, without considering encoder and decoder's interfaces to source and receiver.

%NOTE: here he said something on asimptotic equipartition

\section{\acl{DMC}}

For now, the channel is a black box.
Let $\X$ be the input alphabet and $\Y$ be the output alphabet for the strings that respectively enter and exit the channel.
Select an input symbol $x \in \X$; a random symbol $y \in \Y$ comes out of the channel.

We can think that there is a ``devil'' inside the black box.
The devil has dices that have $\abs{\Y}$ faces.
Whenever an input signal enters the channel, the devil launches a dice and outputs the symbol on the facet that comes up.
The devil cannot launch any dice he likes; instead, there is a mapping between dices and input symbols (you can think of $x$ as the name of the dice that the devil has to use).
So we can influence the devil in using some specific dice (out of the $\abs{\X}$ dices), but the outcome is still stochastic: the probability distribution of each dice is arbitrary.
Note that the channel is usable if we have different dices; otherwise, the output is completely uncontrollable.

Let $W(y|x)$ be the probability that the dice named $x$ will fall onto its facet $y$, with the following conditions: 
\begin{itemize}
	\item $W(y|x) \ge 0$,
	\item $\forall x . \sum_{y \in \Y} W(y|x) = 1$.
\end{itemize}
We can think of $W$ as a matrix whose columns are indexed by $y$ and rows are indexed by $x$.

To communicate iteratively pick $x_1, x_2, \dots, x_n \in \X^n$ and launch a sequence of dices.
There is a probability distribution for $y_1, y_2, \dots, y_n \in Y^n$. Consider the function
\begin{equation*}
	W^n : \X^n \to \Y^n,
\end{equation*}
defined as
\begin{equation*}
	W^n \left( \str{x}, \str{y} \right)
	= \prod_{i = 1}^n W \left( y_i | x_i \right).
\end{equation*}

We assume that every symbol is independently modified (lack of memory of the channel).
Our \ac{DMC} (over time instants) is defined by the set $\left\{ W^n \right\}_{n = 1}^{\infty}$.

\section{Shannon's Noisy Channel Theorem}

The encoder takes the messages of the source and maps it to an arbitrary subset $\C \subseteq \X^n$.
So we have $\abs{\C}$ many messages that can flow in the channel; they can be non-binary strings, but to have an approximation of their length we can assume they are binary.
Then, the length of these messages is $\logtwo{\abs{\C}}$.
We define the speed of the channel as
\begin{equation*}
	\frac{1}{n} \cdot \logtwo{\abs{\C}},
\end{equation*}
where $n$ is the length of transmission in some unit of time.
We can call this \emph{rate} of the code.

Now we are interested in the quality of transmission, that is how well data transmitted can be recovered.
The decoder is a function $\phi: \Y^n \to \C$.
An error event happens when $\str{x} \in \X^n$ is transmitted and $\phi(\str{y}) \neq \str{x}$ is received.
Define
\begin{equation*}
	W^n \left( T|\str{x} \right)
	=
	\sum_{\str{y} \in T} W^n \left( \str{y}|\str{x} \right),
\end{equation*}
where $T \subseteq \Y$.
The error probability of a string $\str{x} \in \X^n$ is
\begin{equation*}
	1 - W^n \left( \phi^{-1}(\str{x}) | \str{x} \right)
	=
	W^n \left( \overline{\phi^{-1}(\str{x})} | \str{x} \right),
\end{equation*}
where
\begin{equation*}
	\phi^{-1}(\str{x}) = \left\{ \str{y} : \phi(\str{y}) = \str{x} \right\}.
\end{equation*}

Define the (maximum) error probability of the code $(\C_n, \phi_n)$ ($W^n$ is fixed) as 
\begin{equation*}
	e_m \left( W^n, \C_n, \phi_n \right)
	=
	\max_{\str{x} \in \C} W^n \left( \overline{\phi^{-1}(\str{x})}, \str{x} \right).
\end{equation*}

\begin{prop}[Achievable rate]
	$R \geq 0$ is an \emph{achievable rate} of transmission over the \ac{DMC} $\{ W \}$ if exists $\{(\C_n, \phi_n)\}_{n=1}^\infty$ such that:
	\begin{itemize}
		\item $\lim_{n \to \infty} \frac{1}{n} \logtwo{\abs{\C_n}} \ge R$;
		\item $\lim_{n \to \infty} e_m(W, \C_n, \phi_n) = 0$.
	\end{itemize}
\end{prop}

What is the highest achievable rate?
\begin{equation*}
	\C \subseteq \X^n \implies \frac{1}{n} \logtwo{\abs{C}} \le \logtwo{\abs{\X^n}}.
\end{equation*}
So the maximum achievable rate is $\logtwo{\abs{\X^n}}$.
Also, if $R_t$ is a series of achievable rates and $R_t$ tends to $R$, then $R$ is an achievable rate too (simple analysis, will not be demonstrated).

Shannon wondered about the highest achievable rate $C(W)$ of a given DMC $\{W\}$.
He called this number \emph{capacity of the channel}.
The answer came out of his intuition but it has been proved true by his students and co-workers.

Let $x \in \X$ be a symbol chosen following a probability distribution $P|\X$.
The input symbol will be sent through the channel $W$ and the output is conditioned by $W$.
Let $Y$ be a \ac{RV} $Y \in \Y$ representing the output of $W$ with respect to input $X$.
Then, we have a joint distribution $P, W | \X \times \Y$.
The probability that a specific input $x$ corresponds to a received symbol $y$ is
\begin{equation*}
	\Pr{X = x, Y = y} = P(x) W(y|x).
\end{equation*}

Define $\MutualInformation{X \land Y}$ to be the number of bits that one can transmit over a channel in a unit of time.
Intuitively, it is what $y$ retains of input $x$ (mutual information).
We can maximize $I$ by controlling the probability distribution $P|\X$.
With some abuse of notation:
\begin{equation*}
	\max_{P|\X} \MutualInformation{P, W}.
\end{equation*}
What is the formula for $I$?
\begin{align*}
	\MutualInformation{X \land Y}
	& =
	\sum_{(x, y) \in \X \times \Y}
	\Pr{X=x, Y=y} \cdot
	\logtwo{\frac{\Pr{X=x, Y=y}}{\Pr{X=x} \cdot \Pr{Y=y}}}
	\\
	& =
	\sum_{x \in \X}
	\sum_{y \in \Y}
	P(x) \cdot W(y|x) \cdot
	\logtwo{\frac{P(x) \cdot W(y|x)}{\sum_{z \in X} P(x) \cdot W(y|z)}}
	\\
	& =
	\MutualInformation{P, W}.
\end{align*}
We used the fact that $\Pr{Y = y} = \sum_{x \in \X} \Pr{Y = y | X = x} = \sum_{x \in \X} W(y|x)$.

\begin{thm}[Shannon's Noisy Channel Theorem]\label{thm:nct}
	The highest achievable rate, given a \ac{DMC} $W$, is
	\begin{equation*}
		C(W) = \max_{P|X} \MutualInformation{P, W}.
	\end{equation*}
\end{thm}

Notice that $C(W)$ is 0 if $X$ and $Y$ are independent, \ie all rows in $W$ are the same. 

\section{\acl{BSC}}

A \ac{BSC} is used to transmit binary strings and the \ac{DMC} has the form
\begin{equation*}
	W = 
	\begin{pmatrix}
		1-p & p \\
		p & 1-p
	\end{pmatrix},
\end{equation*}
with $\X = \Y = \{0,1\}$.

It is called symmetric because $W$ is symmetric.
In this context $P|\X$ is called \emph{crossover} probability.
The capacity of this channel according to \cref{thm:nct} is
\begin{equation*}
	\MutualInformation{X \wedge Y}
	=
	\Entropy{Y} - \Entropy{Y|X}
	\le
	1 - \entropy{p}.
\end{equation*}

This is true because:
\begin{itemize}
	\item the maximum value for $\Entropy{Y}$ is 1;
	\item the following equation holds:
		\begin{equation*}
			\Entropy{Y | X} =
			\sum_{x \in \X} \Pr{X = x} \cdot \underbrace{\Entropy{Y | X = x}}_{\entropy{p}} =
			\entropy{p},
		\end{equation*}
		where
		\begin{align*}
			\Entropy{Y | X = x}
			& =
			\sum_{y \in \Y} \overbrace{\Pr{Y = y | X = x}}^{W(y|x)} \cdot
			\logtwo{\frac{1}{\Pr{Y = y | X = x}}}
			\\
			& =
			\sum_{y \in \Y} W(y | x) \cdot \logtwo{\frac{1}{W(y | x)}}
			\\
			& =
			\entropy{p}.
		\end{align*}
\end{itemize} 

Is this upper bound achievable? We must make sure that
\begin{equation*}
	\exists X : \Entropy{Y} = 1.
\end{equation*}
We can impose the uniform distribution when picking the input values, \ie $\Pr{X = 0} = \frac{1}{2}$.

\begin{obs}[Achievable rate in \acl{BSC}]
	$1-\entropy{2p}$ is an achievable rate if $p < \frac{1}{4}$.
\end{obs}
We will do this using \acp{ECC}.

\begin{proof}
	Choose arbitrary $\str{x} \in \{0, 1\}^n$, but think of $\zero$ (it's easier).
	Consider the Hamming ball $\hball{\zero}{n(p+\epsilon)}$; we want that
	\begin{equation*}
		W^n \left( \phi^{-1}_n(\zero) | \zero \right) \to 1.
	\end{equation*}
	Consider a code for which
	\begin{equation*}
		\forall \str{x} \in \C_n .
		\phi^{-1}_n(\str{x}) \supseteq \hball{\str{x}}{n(p+\epsilon)};
	\end{equation*}
	these Hamming balls should be disjoint.
	We wonder if it is true that
	\begin{equation*}
		W^n \left( \hball{\zero}{n(p+\epsilon)} | \zero \right) \to 1.
	\end{equation*}
	This is what we should achieve, and prove that $np$ is a good choice.
	What we receive is $\str{x} \xor Z^n$, for some \ac{RV} $Z^n$, defined as $Z^n = \str{x} \xor Y^n$.
	\begin{equation*}
		Y^n \in \hball{\zero}{n(p+\epsilon)} \iff Z^n \text{ has at most } n(p+\epsilon) \text{ 1s}. 
	\end{equation*}

	The number of 1s in $Z^n$ is equal to $\sum_{i = 1}^{n} Z_i$, where $Z_i \sim (1-p, p)$ is a \ac{RV}.

	\begin{equation*}
		\expectation{Z_i} = \Pr{Z_i = 0} \cdot 0 + \Pr{Z_i = 1} \cdot 1 = p.
	\end{equation*}

	$\{Z_i\}_{i=1}^n$ is an \ac{IID} sequence of \acp{RV}.
	\begin{equation*}
		\expectation{Z^n} =
		\expectation{\sum_{i = 1}^{n} Z_i} =
		\sum_{i = 1}^{n} \expectation{Z_i} =
		n \cdot p.
	\end{equation*}

	By the law of large numbers,
	\begin{equation*}
		\Pr{\abs{\frac{1}{n} \sum_{i = 1}^{n} Z_i - p} > \epsilon} \to 0.
	\end{equation*}
	The complement of this event has a probability that converges to 1.
	How can we guarantee that these Hamming balls of radius $n(p+\epsilon)$ are disjoint?
	This is guaranteed if $\hdist{\str{x}',\str{x}''} \ge 2n (p+\epsilon)$.
	Thus $\hdist{\C_n} \ge 2n(p+\epsilon)$.
	The Gilbert-Varhsamov bound (\cref{thm:gilbert-varshamov}) says that, if $\delta < \frac{1}{2}$,
	\begin{equation*}
		\exists \C_n : \abs{\C_n} \gtrsim 2^{n \, (1-\entropy{\delta})},
	\end{equation*}
	and that
	\begin{equation*}
		\hdist{\C_n} \gtrsim n \delta.
	\end{equation*}

	We want that $d_H(\C_n) \ge 2n(p+\epsilon)$, thus what we want is achievable if and only if
	\begin{equation*}
		2p + \epsilon < \dfrac{1}{2} \sim p < \dfrac{1}{4}. \qedhere
	\end{equation*}
\end{proof}

Shannon stated something stronger, namely that the capacity is $1 - \entropy{p}$.
Shannon uses ``maximum likelihood''.
He chooses $2^{n \, R}$ strings at random from $T^n_p$, given $P|\X$.

\begin{thm}[Converse part of Shannon's Noisy Channel Theorem]
	If $R$ is such that $\exists \{\C_n, \phi_n\}_{n=1}^\infty$ with
	\begin{equation*}
		\limsup_{n \to \infty}
		\frac{1}{n} \cdot \logtwo{\abs{\C_n}} \ge R,
	\end{equation*}
	and
	\begin{equation*}
		\lim_{n \to \infty} e_n (W^n, \C_n, \phi) = 0,
	\end{equation*}
	then
	\begin{equation*}
		R \le \max_{
			\substack{
				X \in \X : \\
				P_{Y|X} = W
			}
		}
		\MutualInformation{X \land Y}.
	\end{equation*}
\end{thm}

\begin{proof}
	Let $M_n$ be a \ac{RV} uniformly distributed over $\C_n$.
	Take $\frac{1}{n} \cdot \Entropy{M_n}$, the average entropy.
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n} =
		\frac{1}{n} \cdot \logtwo{\abs{\C_n}}.
	\end{equation*}
	To get the upper bound, we manipulate the \ac{LHS}.
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n} =
		\frac{1}{n} \cdot \left[ \Entropy{M_n} - \Entropy{M_n | \phi_n(Y^n)} \right] +
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y^n)},
	\end{equation*}
	where $Y^n$ is the \ac{RV} defined by $P_{Y^n | M_n} = W^n$.
	In the square brackets we have a mutual information, so
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n} =
		\frac{1}{n} \cdot \MutualInformation{M_n \land \phi_n(Y^n)} +
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y^n)}.
	\end{equation*}
	We should be able to prove that this is not more than the upper bound.
	We expect the other therm to be negligible; with high probability the result of decoding the received codeword is the codeword that was sent.
	\begin{equation} \label{eq:mutual-info-processing}
		\MutualInformation{M_n \land \phi_n(Y^n)} \le
		\MutualInformation{M_n \land Y^n}.
	\end{equation}
	This is from the fact that information cannot be gained by processing the \acp{RV}.

	To prove this, first we look at
	\begin{equation} \label{eq:dis1}
		\MutualInformation{M_n \land \phi_n(Y^n)} \le
		\MutualInformation{M_n \wedge \phi_n(Y^n), Y^n}.
	\end{equation}
	By definition of mutual information, \cref{eq:dis1} can be written as
	\begin{equation*}
		\Entropy{M_n} - \Entropy{M_n | \phi_n(Y^n)} \le
		\Entropy{M_n} - \Entropy{M_n | \phi_n(Y^n), Y^n},
	\end{equation*}
	and since $\Entropy{A | B} \ge \Entropy{A | B, C}$ we have that
	\begin{equation*}
		\Entropy{M_n | \phi_n(Y^n)} \ge
		\Entropy{M_n | \phi_n(Y^n), Y^n}.
	\end{equation*}
	Then \cref{eq:dis1} follows.

	The \ac{RHS} of \cref{eq:dis1} can be written as
	\begin{align*}
		\MutualInformation{M_n \land \phi_n(Y^n), Y^n}
		& =
		\MutualInformation{M_n \land Y^n} +
		\underbrace{\MutualInformation{M_n \land \phi_n(Y^n) | Y^n}}_{0}
		\\
		& =
		\MutualInformation{M_n \land Y^n}.
	\end{align*}
	We used the fact that
	\begin{equation*}
		\MutualInformation{M_n \land \phi_n(Y^n) | Y^n} \le
		\Entropy{\phi_n(Y^n) | Y^n} = 0.
	\end{equation*}
	Combining these two things, we have obtained \cref{eq:mutual-info-processing}.

	What we wrote as $M_n$ is just a vector of \acp{RV}, \ie something like $X_1 \dots X_n = X^n$.
	Thus,
	\begin{equation*}
		\MutualInformation{M_n \land Y^n} =
		\MutualInformation{X^n \land Y^n}.
	\end{equation*}
	This is what we want to upper bound with the conjectured value of capacity.

	By definition of information gain, we write
	\begin{equation*}
		\MutualInformation{X^n \land Y^n} =
		\Entropy{Y^n} - \Entropy{X^n | Y^n} \le
		\sum_{i = 1}^n \Entropy{Y_i} - \Entropy{Y^n | X^n}.
	\end{equation*}
	Now we rewrite $\Entropy{Y^n | X^n}$ using the chain rule (\cref{prop:chain-rule}):
	\begin{equation*}
		\Entropy{Y^n | X^n} =
		\sum_{i = 1}^n \Entropy{Y_i | X^n, Y_1, \dots, Y_{i-1}}.
	\end{equation*}

	Since the channel we are using is a \ac{DMC} (and thus it is memoryless), we have that
	\begin{equation*}
		\Entropy{Y_i | X^n, Y_1, \dots, Y_{i-1}} =
		\Entropy{Y_i | X_i},
	\end{equation*}
	so we can rewrite what we have broken up with the chain rule as
	\begin{align*}
		\Entropy{Y^n | X^n}
		& =
		\sum_{i = 1}^n \Entropy{Y_i | X^n, Y_1, \dots, Y_{i-1}}
		\\
		& =
		\sum_{i = 1}^{n} \Entropy{Y_i | X_i}.
	\end{align*}

	Using the fact that we are using a \ac{DMC}, we have obtained that
	\begin{align*}
		\MutualInformation{X^n \land Y^n}
		& =
		\sum_{i = 1}^{n} \MutualInformation{X_i \land Y_i}.
		\\
		& \le
		n \cdot \max_{P_{Y|X} = W} \MutualInformation{X \land Y},
	\end{align*}
	which is $n$ times capacity, so we can write this as
	\begin{equation*}
		\frac{1}{n} \cdot \MutualInformation{M_n \land \phi_n(Y^n)} \le C(W).
	\end{equation*}

	Now we are left to prove that
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y^n)} \to 0.
	\end{equation*}
	We will do this using Fano's inequality.

	We introduce the \ac{RV} $Z_n$, defined as
	\begin{equation*}
		Z_n =
			\begin{cases}
				1 \text{ if } \phi_n(Y^n) \neq M_n,\\
				0 \text{ otherwise.}
			\end{cases} 
	\end{equation*}

	We can say that
	\begin{equation*}
		\Entropy{M_n | \phi_n(Y^n)} \le
		\Entropy{M_n, Z_n | \phi_n(Y^n)}
	\end{equation*}
	since we ``added'' something to a \ac{RV}.

	Now we find an upper bound for it:
	\begin{align*}
		\Entropy{M_n, Z_n | \phi_n(Y^n)}
		& =
		\Entropy{Z_n | \phi_n(Y^n)} +
		\Entropy{M_n | \phi_n(Y^n), Z_n}
		\\
		& \le
		\Entropy{Z_n} +
		\Entropy{M_n | \phi_n(Y^n), Z_n}
		\\
		& \le
		1 +
		\Entropy{M_n | \phi_n(Y^n), Z_n}.
	\end{align*}
	This means that
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y^n)} \le
		\frac{1}{n} + \frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y_n), Z_n}.
	\end{equation*}
	Since $\frac{1}{n} \to 0$, we are left to show that
	\begin{equation*}
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y_n), Z_n}
	\end{equation*}
	is small, \ie it goes to $0$ too.

	We can break this up:
	\begin{equation} \label{eq:eqbreak}
	\begin{aligned}
		\frac{1}{n} \cdot \Entropy{M_n | \phi_n(Y_n), Z_n}
		= &~
		\Pr{Z_n = 1} \cdot \Entropy{M_n | \phi_n(Y^n), Z_n = 1}
		\\
		&~+
		\Pr{Z_n = 0} \cdot \Entropy{M_n | \phi_n(Y^n), Z_n = 0}.  
	\end{aligned}
	\end{equation}

	For the case in which $Z_n = 1$, we bring in the error probability:
	\begin{align*}
	  \epsilon_n
	  & =
	  \Pr{Z_n = 1} =
	  \Pr{M_n \neq \phi_n(Y^n)}
	  \\
	  & \le
	  e_n(W^n, \C_n, \phi_n) \to 0.
	\end{align*}
	So $\epsilon_n \to 0$, since the average is not more than the maximum.
	\begin{align*}
		\Entropy{M_n | \phi_n(Y^n), Z_n = 1}
		& =
		\\
		\sum_{y \in \Y^n} \Pr{Y^n = y}
		& \cdot
		\Entropy{M_n | \phi_n(Y^n) = \phi_n(y), Z_n = 1}.
	\end{align*}
	We have that $M_n \in \X^n$, so its entropy is no more than $\logtwo{\abs{\X^n}}$.
	The first term of \cref{eq:eqbreak} can thus be bounded by
	\begin{equation*}
		\epsilon_n \cdot
		\frac{1}{n} \cdot
		\Entropy{M_n | \phi_n(Y^n), Z_n = 1}
		\le
		\epsilon_n \cdot
		\frac{1}{n} \cdot
		\logtwo{\abs{\X^n}}
		=
		\epsilon_n \cdot
		\logtwo{\abs{\X}}
		\to 0.
	\end{equation*}

	The second term of \cref{eq:eqbreak} is equal to 0.
	\begin{equation*}
		\frac{1}{n} \cdot \Pr{Z_n = 0} \cdot \Entropy{M_n | \phi_n(Y^n), Z_n = 0}.
	\end{equation*}
	To see why, note that there is no error, so $M_n = \phi_n(Y^n)$, and thus $\Entropy{M_n | \phi_n(Y^n), Z_n} = 0$.
\end{proof}


\section{Zero Error Capacity}

Zero stands for zero probability, so the probability of error is bound to be equal to 0.
We translate this problem in graph theory: we will look
at product of graphs and powers of graphs.
We have a \ac{DMC} $\{W\}$, $W : \X \to \Y$.
$W^n : \X^n \to \Y^n$, the mathematical model of a code is the very same.
A code is $(\C_n, \phi)$ with $\C_n \subseteq \X^n$ and $\phi: \Y^n \to \C_n$.
A code has two parameters:
\begin{itemize}
	\item $\frac{1}{n} \cdot \abs{\C_n}$ is the size of the code compared to $n$.
		It is the number of bits transmitted over the channel per use of the channel.
	\item $e_n(W^n, \C_n, \phi) = \max_{\str{x} \in \C_n} W^n \left( \overline{\phi^{-1}(\str{x})} | \str{x} \right)$ is the probability of error.
\end{itemize}

$(\C_n, \phi)$ is a zero error code if $e_n(W^n, \C_n, \phi) = 0$.
$R \geq 0$ is an achievable rate for error probability $0$ if $\exists (\C_n, \phi_n)$, $\C_n \subseteq \X^n$ with
\begin{equation*}
	\limsup_{n \to \infty} \frac{1}{n} \cdot \logtwo{\abs{\C_n}} \ge R,
\end{equation*}
and
\begin{equation*}
	e_n(W^n, \C_n, \phi_n) = 0.
\end{equation*}

If $W(y|x) > 0$ for all $x, y$ then this is not possible.
If you have 0s in the matrix, we are only interested in the patterns of the 0s.
To have 0 error, we should have that, for all $\str{x} \in \C_n$,
\begin{equation*}
	W^n \left( \phi^{-1}(\str{x}) | \str{x} \right) = 1.
\end{equation*}

The decoding function must be fixed if we want $0$ error.
If $W^n(\str{y} | \str{x}) \ge 0$, we must have that $\str{y} \in \phi^{-1}(\str{x})$.
Given $\str{x}$, the set $\left\{ \str{y} | W^n \left( \str{y} | \str{x} \right) > 0 \right\}$ is uniquely defined.
This is the support of conditional distribution:
\begin{equation*}
	\supp(\str{x}) = \left\{ \str{y} |  W^n \left( \str{y} | \str{x} \right) > 0 \right\}.
\end{equation*}

We should have that, for all $\str{x} \in \C_n$,
\begin{equation*}
	\supp(\str{x}) \subseteq \phi^{-1}(\str{x}).
\end{equation*}

So what we want to achieve, \ie how to get the 0 error probability, only depends on the set $\C_n$.
Support sets have to be disjoint.
$\C_n$ is a $0$ error code if
\begin{equation*}
	\forall \{ \str{x}',\str{x}'' \} \in \binom{\C_n}{2} .
	\supp(\str{x}') \cap \supp(\str{x}'') = \emptyset
\end{equation*}

\begin{obs}[Support set in $\X^n$]
	Let $\str{x} \in \X^n$.
	Then
	\begin{equation*}
		\str{y} \in \supp(\str{x}) \iff W^n \left( \str{y} | \str{x} \right) >0,
	\end{equation*}
	but $W^n$ is a product of probabilities; thus, since
	\begin{equation*}
		W^n \left( \str{y} | \str{x} \right) = \prod_{i=1}^{n} W \left( y_i | x_i \right),
	\end{equation*}
	it must be that, for all $i$,
	\begin{equation*}
		W(y_i, x_i) > 0.
	\end{equation*}
	So the support set is
	\begin{equation*}
		\supp(\str{x}) = \bigtimes_{i=1}^{n} \supp(x_i).
	\end{equation*}
\end{obs}

This is a combinatorial condition.
The support is the set of positive elements of the row of $x$.
You need at least two orthogonal rows in the matrix.

\begin{obs}[Disjoint support sets]
	\begin{equation*}
		\supp(\str{x}') \cap \supp(\str{x}'') = \emptyset
		\iff
		\supp(x_i') \cap \supp(x_i'') = \emptyset,
	\end{equation*}
	for some $i$. 
\end{obs}

\begin{proof}
	Suppose 
	\begin{align*}
		\supp(\str{x}') & \cap \supp(\str{x}'') \neq \emptyset
		\\
		& \iff
		\\
		\exists \str{z} \in \supp(\str{x}') & \cap \supp(\str{x}'')
		\\
		& \iff
		\\
		\forall i . 
		z_i \in \supp(x_i') & \cap \supp(x_i''). \qedhere
	\end{align*}
\end{proof}

Consider the graph $G = G(W)$ such that $V(G) = \X$ and $\{x', x''\} \in E(G)$ if $\supp(x') \cap \supp(x'') = \emptyset$.
We are looking for a clique in this graph, since its size $\omega(G)$ is the size of the best code one can have.

What about when we use the channel more than once?
We look at $G^n = G(W^n)$, where $V(G^n) = \X^n$ and $\{\str{x}', \str{x}''\} \in E(G^n)$ if $\exists i : \supp(x_i') \cap \supp(x_i'') = \emptyset$.
This sequence of graphs can be built using just the first graph, and forgetting about the matrix.

Take $G$.
If $\{a, b\} \in E(G)$ they are indistinguishable.
Now take $G^n$.
$\{\str{a}, \str{b}\} \in E(G^n)$ are indistinguishable if $\exists i : \{a_i, b_i\} \in E(G)$ with $a = a_1 \dots a_n$ and $b = b_1 \dots b_n$.
Every clique of maximal length is a 0 error code.

We say that 
\begin{equation*}
	\limsup_{n \to \infty} \frac{1}{n} \cdot \logtwo{\omega(G^n)}
\end{equation*}
is the zero error capacity of $G$.
This limit exists for every graph.

\begin{prop}[Noisy channels and graphs]
	For every graph $G$, $\exists W : G = G(W)$.
\end{prop}

\begin{proof}
	Consider a matrix $W$ where the rows are the vertexes of the graphs and columns are indexed by the non-edges, \ie the pairs of $\binom{V(G)}{2}$ that are not in $E(G)$.
	\begin{equation}
		W \left( \{a,b\} | x \right) =
			\begin{cases}
				0 \text{ if } x \not\in \{a, b\}, \\
				1 \text{ if } x \in \{a, b\}.
			\end{cases}
	\end{equation}
	If the rows of $x$ and $y$ are not orthogonal, $\{x, y\}$ is not an edge.
	There are two problems:
	\begin{itemize}
		\item rows could not sum to 1;
		\item rows could sum up to 0.
	\end{itemize}
	The first problem is avoided by dividing the rows.
	The second is a avoided by doing
	\begin{equation*}
		W = (W I)
	\end{equation*}
	\ie by appending the identity matrix to $W$.
	Now $W$ can be normalized, and $G = G(W)$.
\end{proof}

$\sqrt[n]{\omega(G^n)}$ is how much larger is $\omega(G^n)$ with respect to $\omega(G)$.
For graphs of $5$ vertices there is only one graph for which
\begin{equation*}
	\sqrt[n]{\omega(G^n)} \neq \omega(G).
\end{equation*}
The graph is shown in \cref{fig:5-graph}.

\begin{figure}
	\centering
	\includegraphics[width=100px]{pictures/graph5v.eps}
	\caption{$C_5$. \label{fig:5-graph}}
\end{figure}

Lov\`asz gave a function for $ \sqrt[n]{\omega(G^n)}$ in '79.
In fact it works for graphs that are self complementary and vertex transitive.

The Shannon capacity of $G$ is defined as
\begin{equation*}
	C(G) = \limsup_{n \to \infty} \sqrt[n]{\omega(G^n)}.
\end{equation*}

Shannon proved that
\begin{equation*}
	\sqrt{5} \le C(C_5) \le \frac{5}{2}.
\end{equation*}
Lov\`asz proved that in fact $C(C_5) = \sqrt{5}$.
For the upper bound we have that $C_5 = G(W)$ with
\begin{equation*}
	W =
		\begin{pmatrix}
			\frac{1}{2} & \frac{1}{2} & 0 & 0 & 0 \\
			0 & 0 & \frac{1}{2} & \frac{1}{2} & 0 \\
			\frac{1}{2} & 0 & 0 & 0 & \frac{1}{2} \\
			0 & \frac{1}{2} & \frac{1}{2} & 0 & 0 \\
			0 & 0 & 0 & \frac{1}{2} & \frac{1}{2} \\
		\end{pmatrix}.
\end{equation*}

Then he said that 0 error capacity is less than or equal to the capacity.
Shannon noticed, for the lower bound, that $C(G) > \omega(G)$, but $\omega(G) = 2$.
If you look at $G^2$, \ie $C_5^2$, then $\omega(C_5^2) = 5$.
\begin{equation*}
	\omega(G^n) \ge [\omega(G)]^n \implies
	\sqrt[n]{\omega(G^n)} \ge \omega(G).
\end{equation*}
Using the product set you only get 4.
But Shannon noticed that these $5$ strings are distinguishable:
\begin{equation*}
	\{00, 11, 24, 31, 43\}.
\end{equation*}

\begin{lem}[Fekete]
	Take a sequence $a_n \in \Reals$.
	If the sequence is super additive, \ie for all $m, n \in \Naturals$ we have that $a_{m+n} \ge a_m + a_n$, and if for all $n \in \Naturals$
	\begin{equation*}
		\frac{a_n}{n} \le M,
	\end{equation*}
	then
	\begin{equation*}
		\exists \lim_{n \to \infty}  \frac{a_n}{n} 
	\end{equation*}
	and
	\begin{equation*}
		\lim_{n \to \infty}  \frac{a_n}{n} = \sup_n \frac{a_n}{n}.
	\end{equation*}
\end{lem}

To apply this to $\omega(G^n)$ we have to take the logarithm, since $\omega(G^n)$ is super multiplicative (and thus $\logtwo{\omega(G^n)}$ is super additive).
We have that $\omega(G^n) \le \abs{V(G)}^n$, and thus
\begin{equation*}
	\frac{\logtwo{\omega(G^n)}}{n} \le \logtwo{\abs{V(G)}}.
\end{equation*}
Since $\frac{a_n}{n} \le M$, $\exists \hat{M} = \sup_n \frac{a_n}{n}$ with $\hat{M} \le M$.
So, for all $\epsilon > 0$, $\exists n_0$ such that
\begin{equation*}
	\frac{a_{n_0}}{n_0} > \hat{M} - \epsilon.
\end{equation*}

Take arbitrary $n > n_0$ with $n = q_n \cdot n_0 + r_n$, with $0 \le r_n < n_0$.
\begin{align*}
	\frac{a_n}{n}
	& =
	\frac{a_{q_n \cdot n_0 +r_n}}{q_n \cdot n_0 +r_n}
	\\
	& \ge
	\frac{q_n \cdot a_{n_0} + a_{r_n}}{(q_n+1) \, n_0}
	\\
	& =
	\frac{q_n}{q_{n+1}} \cdot
	\frac{a_{n_0}}{n_0} +
	\frac{a_{r_n}}{(q_n + 1) \, n_0}.
\end{align*}
So the limit can be bounded from below:
\begin{equation*}
	\liminf_{n \to \infty} \frac{a_n}{n} \ge
	\liminf_{n \to \infty} \left(
		\frac{q_n}{q_{n+1}} \cdot \frac{a_{n_0}}{n_0} +
		\frac{a_{r_n}}{(q_n + 1) \, n_0}
	\right) \ge
	\hat{M} - \epsilon.
\end{equation*}
Since $a_{r_n} \ge \min \{a_0, \dots,a_{n_0 -1}\}$, the last fraction goes to 0.

\begin{prop}[Graph capacity]
	\begin{equation*}
		\omega(G) \leq C(G) \leq \chrom{G},
	\end{equation*}
	where $\chrom{G}$ is the chromatic number of $G$.
\end{prop}

A colouring of $G$ is a function $f : V(G) \to C$ such that if $\{a, b\} \in E(G)$ then $f(a) \neq f(b)$.
$\chrom{G}$ is the minimum cardinality of $C$ such that such a function exists.

\begin{proof}
	\begin{equation*}
		\left[ \omega(G) \right]^n \le \omega(G^n) \le \chrom{G^n}.
	\end{equation*}
	The first part comes from super-multiplicativity of $\omega(G)$, the second from the fact that the chromatic number of a graph is greater than the largest clique.
	Since $\chrom{G}$ is sub-multiplicative, we have
	\begin{equation*}
		\chrom{G^n} \le \left[ \chrom{G} \right]^n.
	\end{equation*}

	To show that $\chrom{G}$ is sub-multiplicative, consider an optimal colouring of $G$:
	\begin{equation*}
		f : V(G) \to C \text{ with } \abs{C} = \chrom{G}.
	\end{equation*}
	Take $\str{x} \in V(G^n)$, we colour it separately, for $x_i$ in $x_1 \dots x_n$.
	The function $f^n(\str{x})$, \ie its extension by concatenation, is a correct colouring.

	To see why, note that if $\{\str{x}, \str{y}\} \in E(G^n)$, it must be that $\exists i : \{x_i, y_i\} \in E(G)$, but then, since $x_i$ and $y_i$ are adjacent in $G$, their colourings are different, \ie $f(x_i) \neq f(y_i)$, and consequently also the colourings of $\str{x}$ and $\str{y}$, \ie $f^n(\str{x}) \neq f^n(\str(y))$.

	Furthermore, the number of colours used by $f^n$ is no more than $\abs{C^n} = \left[ \chrom{G} \right]^n$, so
	\begin{equation*}
		\omega(G) \le \sqrt[n]{\omega(G^n)} \le \chrom{G}. \qedhere
	\end{equation*}
\end{proof}

There is a simple upper bound that is better than this.
We use the ``fractional'' chromatic number, by expressing colouring as a \ac{ILP} problem and by dropping the integer constraint.

\begin{cor}[Graph capacity of perfect graphs]
	\begin{equation*}
		\omega(G) = \chrom{G} \implies C(G) = \omega(G).
	\end{equation*}
\end{cor}

The corollary doesn't give us much.
You can make any graph like this.
Schutzenberger + Berge showed some classes of graphs for which this holds.
One example is graphs for which vertices are intervals, and intersecting intervals share and edge.
Furthermore, for interval graphs, $\forall G' \subseteq G$ induced subgraph we have that $\omega(G') = \chrom(G')$.

\begin{definition}[Perfect graph] \label{definition:perfect-graph}
	$G$ is perfect if
	\begin{equation*}
		\omega(G') = \chrom{G'}\ \forall G' \subseteq G,
	\end{equation*}
	where $G'$ is an induced subgraph.
\end{definition}

\begin{definition}[Minimally imperfect graph]
A graph $G$ is minimally imperfect if
	\begin{enumerate}
		\item $G$ is not perfect, \ie $\omega(G) < \chrom{G}$,
		\item $\forall G' \subseteq G$ induced subgraph, $G'$ is perfect, \ie $\omega(G') = \chrom{G'}$.
	\end{enumerate}
\end{definition}

What makes \cref{definition:perfect-graph} beautiful are two conjectures, by Berge.
By now they are both theorems.
\begin{itemize}
	\item \textbf{Weak}: $G$ is perfect if and only if $\overline{G}$ is perfect.
	\item \textbf{Strong}: minimally imperfect graphs are either odd cycles or their complements.
\end{itemize}





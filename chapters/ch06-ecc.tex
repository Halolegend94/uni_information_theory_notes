
\chapter{\aclp{ECC}}

\acp{ECC} arise in probabilistic contexts.
Consider a memory device with $n$ cells.
We can model the content as a string $\str{x} \in \{0, 1\}^n$.
The device decays; each cell can flip its value independently from the others with a certain probability (uniform).
The probability $p$ is usually $< 1/2$.
We can expect no more than $np$ flips.
Can we guarantee recovery from so many errors?
Yes, thanks to algebra.
The string $\str{x}$ can be converted into anything inside a ball of radius $np$.
We want that two strings $\str{x}$ and $\str{y}$ are distant, so that their balls do not intersect.
The pairwise distance of the words should be $\ge 2np + 1$.

Let $\C \subseteq \{0, 1\}^n$ be a codeblock.
The first property that we want from $\C$ is that $\abs{\C}$ should be large.

Next, we extend the definition of Hamming Distance to sets, \ie we define
\begin{equation*}
	\hdist{\C} = \min_{\{\str{x}, \str{y}\} \in \binom{\C}{2}} \hdist{\str{x}, \str{y}},
\end{equation*}
where 
\begin{equation*}
	\binom{\C}{2} = \left\{ \{\str{x}, \str{y}\}\ |\ \str{x}, \str{y} \in \C \land \str{x} \neq \str{y} \right\}. 
\end{equation*}
So $\hdist{\C}$ tells how close the closest elements of $\C$ are.
$\hdist{\C}$ should be large too.

What is the best possible trade-off?
\begin{equation*}
	M(n, d) = \max_{\C \subseteq \{0,1\}^{n} : \hdist{\C} \ge d} \abs{\C}
\end{equation*}
This says that we are concentrating on codes for which $\hdist{\C} \ge d$ for some $d$.

We can think of the graph for which $V = \{0, 1\}^n$ and $\str{x}, \str{y} \in \binom{V}{2}$ share an edge if and only if $\hdist{\str{x}, \str{y}} \ge d$.
$M(n,d)$ is the size of the largest clique in this graph.

We try to make this simpler by looking at the problem from an asymptotic point of view.
We consider $M(n, n \delta)$ for $\delta \in [0, 1]$.
This grows to infinity exponentially in $n$, while
\begin{equation*}
	\frac{1}{n} \log_2 \left( M(n, n \delta) \right)
\end{equation*}
does not grow exponentially.
So we look at the superior limit of
\begin{equation*}
	R(\delta) = \limsup_{n \to \infty} \frac{1}{n} \log_2 \left( M(n, n \delta) \right).
\end{equation*}
$R$ here stands for \emph{rate}.

$M(n, n \delta)$ is the size of largest set of strings one can put inside $n$ bits of memory, with minimum distance $n \delta$.
Its logarithm is the number of bits of true information that we can store.
Dividing by $n$ we get the amount of information bit by bit.
We can easily see that $R(0) = 1$ and that $R(1) = 0$.

\begin{thm}[Gilbert-Varshamov bound] \label{thm:gilbert-varshamov}
	If $\delta \in \left[ 0, \frac{1}{2} \right]$, then
	\begin{equation*}
		R(\delta) \ge 1 - \entropy{\delta}.
	\end{equation*}
\end{thm}

\begin{proof}
	This means that
	\begin{equation*}
		M(n, n \delta) \ge 2^{n \, \left[1 - \entropy{\delta} \right]}.
	\end{equation*}
	This bound was later improved to $n 2^{n \, \left[1 - \entropy{\delta} \right]}$.

	For now, fix $n, d \in \Naturals$.
	Take an arbitrary string $\str{x} \in \{0, 1\}^n$, and exclude $\hball{\str{x}}{d-1}$ from our future choices.

	Then, after having found $\str{x}_1, \dots, \str{x}_{t-1}$, we choose arbitrarily
	\begin{equation*}
		\str{x}_t \in \{0, 1\}^n \setminus \bigcup_{i = 1}^{t-1} \hball{\str{x}_i}{d-1},
	\end{equation*}
	so, after picking each string, we exclude the Hamming Ball of radius $d-1$ around it from our future choices.

	How long can this go on?
	Maybe after $m$ steps we have that
	\begin{equation*}
		\{0, 1\}^n \setminus \bigcup_{i = 1}^{m} \hball{\str{x}_i}{d-1} = \emptyset.
	\end{equation*}

	How large $m$ can be?
	We stop when 
	\begin{align*}
		\{0, 1\}^n
		& \subseteq
		\bigcup_{i = 1}^{m} \hball{\str{x}_i}{d-1}
		\\
		& \implies
		\\
		2^n
		& \le
		\abs{\bigcup_{i = 1}^{m} \hball{\str{x}_i}{d-1}}
		\\
		& \le
		\sum_{i = 1}^m \abs{\hball{\str{x}_i}{d-1}}
		\\
		& \le
		\sum_{i = 1}^m \abs{\hball{\str{x}_i}{d}}
		\\
		& \le
		m 2^{n \, \entropy{\frac{d}{n}}},
	\end{align*}
	where the last inequality holds since $d \le \frac{n}{2}$.
	We have proven the thesis, since now
	\begin{equation*}
		M(n, d) \ge m \ge \frac{2^n}{2^{n \, \entropy{\frac{d}{n}}}}
		=
		2^{n \, \left[ 1 - \entropy{\frac{d}{n}} \right]},
	\end{equation*}
	where $\frac{d}{n} = \delta.$
\end{proof}

\begin{thm}[Hamming bound] \label{thm:hamming-bound}
	For $\delta \in [0, 1]$,
	\begin{equation*}
		R(\delta) \le 1 - \entropy{\frac{\delta}{2}}.
	\end{equation*}
\end{thm}

\begin{proof}
	Fix $n, d \in \Naturals$, $\hdist{\C} \ge d$, arbitrarily.

	Consider the centres $\str{x}$ and $\str{y}$ of two Hamming balls, with $\hdist{\str{x}, \str{y}} = d$.
	For the two Hamming balls to be disjoint, we must choose
	\begin{equation*}
		\hball{\str{x}}{\frac{d-1}{2}}
		\text{ and }
		\hball{\str{y}}{\frac{d-1}{2}}.
	\end{equation*}

	Assume the two Hamming Balls are not disjoint, \ie
	\begin{equation*}
		\hball{\str{x}}{\frac{d-1}{2}}
		\cap
		\hball{\str{y}}{\frac{d-1}{2}}
		\neq \emptyset;
	\end{equation*}
	but this means that
	\begin{equation*}
		\exists \str{z} \in
		\hball{\str{x}}{\frac{d-1}{2}}
		\cap
		\hball{\str{y}}{\frac{d-1}{2}},
	\end{equation*}
	and that, since $\str{z}$ is in both Hamming Balls,
	\begin{equation*}
		\hdist{\str{x},\str{z}} \le \frac{d-1}{2}
		\qquad
		\hdist{\str{y},\str{z}} \le \frac{d-1}{2}.
	\end{equation*}
	But then we have that
	\begin{equation*}
		d = \hdist{\str{x},\str{y}}
		\le
		\hdist{\str{x},\str{z}} + \hdist{\str{y},\str{z}}
		\le
		2 \cdot \frac{d-1}{2}
		= d-1 < d,
	\end{equation*}
	in contradiction with the assumption that $\hdist{\str{x},\str{y}} = d$.

	If all strings have a distance of at least $d$, we can correct up to $\frac{d-1}{2}$ errors.

	Assume $\C$ is built using disjoint balls, and that $M(n, d) = \abs{\C}$.
	Then
	\begin{equation*}
		\abs{\bigcup_{\str{x} \in \C} \hball{\str{x}}{\frac{d-1}{2}}}
		=
		\abs{\C} \cdot \abs{\hball{\zero}{\frac{d-1}{2}}}
		\ge
		\frac{\abs{\C}}{n+1} \cdot 2^{n \, \entropy{\frac{d-1}{2n}}}.
	\end{equation*}
	Now, since the Hamming Balls are disjoint, we have that
	\begin{equation*}
		\abs{\bigcup_{\str{x} \in \C} \hball{\str{x}}{\frac{d-1}{2}}} \le 2^n,
	\end{equation*}
	which in turn gives us that $M(n,d) = \abs{\C}$ can be upper bounded as
	\begin{equation*}
		M(n, d) = \abs{\C} \le (n+1) \cdot 2^{n \, \left[1 - \entropy{\frac{d-1}{2n}}\right]}.
	\end{equation*}
	
	We now take the logarithm and normalise by $n$, and obtain
	\begin{equation*}
		\frac{1}{n} \log_2 \left( M(n, d) \right)
		\le
		\frac{1}{n} \log_2 \left( n+1 \right) + 1 - \entropy{\frac{d-1}{2n}},
	\end{equation*}
	and, by taking the limit superior of that quantity, we get
	\begin{align*}
		R(\delta)
		& =
		\limsup_{n \to \infty} \frac{1}{n} \logtwo{M(n, n \delta)}
		\\
		& =
		\limsup_{n \to \infty}
		\cancelto{0}{
			\frac{1}{n} \logtwo{n+1}
		}
		+ 1 - \entropy{
			\frac{\cancel{n} \, \delta}{2 \, \cancel{n}}
			- \cancelto{0}{\frac{1}{2n}}
		},
		\\
		& \le
		1 - \entropy{\frac{\delta}{2}}. \qedhere
	\end{align*}
\end{proof}

\section{\acl{UD} codes}

\begin{definition}[\acl{UD} code]
	Let $f : \M \to \{0,1\}^{\star}$, and define $f^{\star} : \M^{\star} \to \{0,1\}^{\star}$ as
	\begin{equation*}
		f^{\star} (\str{m}) = f(m_1) f(m_2) \dots f(m_t),
	\end{equation*}
	for $\str{m} \in \M^{\star}$ with $\str{m} = m_1 m_2 \dots m_t$ for some $t$.
	We say that $f$ is \ac{UD} if $f^{\star}$ is injective.
\end{definition}
If $f$ is a prefix code, $f$ is \ac{UD}.

\begin{thm}[Kraft - McMillan theorem on \acl{UD} codes]
	If $f$ is \ac{UD} then Kraft's inequality holds, \ie
	\begin{equation*}
		\sum_{m \in \M} 2^{-\abs{f(m)}} \le 1.
	\end{equation*}
\end{thm}

\begin{proof}
	Let
	\begin{equation*}
		q = \sum_{m \in \M} 2^{-\abs{f(m)}}.
	\end{equation*}

	We would like to prove that $q \le 1$.
	To do so, we consider $q^n$, which will involve the length of concatenations of code words.
	We will show that $q^n$ ``grows slowly'', \ie it does not grow exponentially, and thus is asymptotically less than or equal to 1. 
	\begin{align*}
		q^n
		& =
		\left[ \sum_{m \in \M} 2^{-\abs{f(m)}} \right]^n
		\\
		& =
		\prod_{i = 1}^n \left[ \sum_{m \in \M} 2^{-\abs{f(m)}} \right]
		\\
		& =
		\sum_{\str{m} \in \M^n} \left[ \prod_{i = 1}^n 2^{-\abs{f(m_i)}} \right]
		\\
		& =
		\sum_{\str{m} \in \M^n} 2^{-\abs{f^{\star}(\str{m})}}.
	\end{align*}
	$f^{\star}(\str{m})$ is just a binary string.
	We can break up the summation over the length of these strings, as follows:
	\begin{equation*}
		\sum_{\str{m} \in \M^n} 2^{-\abs{f^{\star}(\str{m})}}
		=
		\sum_{t = n}^{nL}
		\sum_{
			\substack{
				\str{m} \in \M^n : \\
				\abs{f^{\star}(\str{m})} = t
			}
		} 2^{-\abs{f^{\star}(\str{m})}},
	\end{equation*}
	with $L = \max_{m \in \M} \abs{f(m)}$.
	Now we use the fact that $f^{\star}(\cdot)$ is injective: each binary string of length $t$ appears just once in the sum.
	We can't have two strings of messages encoded by the same binary string.
	\begin{equation*}
		\sum_{t = n}^{nL}
		\sum_{
			\substack{
				\str{m} \in \M^n : \\
				\abs{f^{\star}(\str{m})} = t
			}
		} 2^{-\abs{f^{\star}(\str{m})}}
		\le
		\sum_{t = n}^{nL} 2^t \cdot 2^{-t}
		\leq
		nL.
	\end{equation*}
	This follows from the fact that we can have at most $2^{t}$ messages encoded by binary strings of length $t$.

	What we have shown is that $q^n \le nL$, and thus
	\begin{equation*}
		q \le \sqrt[n]{nL}
		=
		\sqrt[n]{n} \sqrt[n]{L}
		\to 1,
	\end{equation*}
	and so $q \le 1$.
\end{proof}

\begin{thm}[Plotkin bound]
	\begin{equation*}
		\delta \ge \frac{1}{2} \implies R(\delta) = 0.
	\end{equation*}
\end{thm}

% Recall that
% \begin{equation*}
% 	M(n, d) = \max_{\C \subseteq \{0, 1\}^n : \hdist{\C} \ge d} \abs{\C},
% \end{equation*}
% with
% \begin{equation*}
% 	\hdist{\C}
% 	=
% 	\min_{\{\str{x}, \str{y}\} \in \binom{\C}{2}} \hdist{\str{x}, \str{y}}.
% \end{equation*}
% The information rate is defined as
% \begin{equation*}
% 	R(\delta) = \limsup_{n \to \infty} \frac{M(n, n \delta)}{n}.
% \end{equation*}
We already know that, if $\delta \leq \frac{1}{2}$,
\begin{equation*}
	1 - \entropy{\delta} \le R(\delta) \le 1 - \entropy{\frac{\delta}{2}}.
\end{equation*}

\begin{proof}
	Let $\C \subseteq \{0, 1\}^n$, chosen arbitrary.
	Let $M = \abs{\C}$.
	\begin{align*}
		\hdist{\C}
		& \le
		\sum_{\left\{ \str{x}, \str{y} \right\} \in \binom{\C}{2}}
		\frac{\hdist{\str{x},\str{y}}}{\binom{M}{2}}
		\\
		& =
		\frac{2}{M (M-1)}
		\sum_{\left\{ \str{x}, \str{y} \right\} \in \binom{\C}{2}}
		\hdist{\str{x},\str{y}}
		\\
		& =
		\frac{2}{M (M-1)}
		\sum_{\left\{ \str{x}, \str{y} \right\} \in \binom{\C}{2}}
		\sum_{i = 1}^n \hdist{x_i, y_i}
		\\
		& =
		\frac{2}{M (M-1)}
		\sum_{i = 1}^n
		\sum_{\left\{ \str{x}, \str{y} \right\} \in \binom{\C}{2}} \hdist{x_i, y_i},
	\end{align*}
	where $\str{x} = x_1 \dots x_n$.

	Now, picture a matrix with $n$ columns and $M = \abs{\C}$ rows.
	Each row is an element of $\C$, each column is a coordinate of $\str{x} \in \C$.
	One can think of $\sum_{\left\{ \str{x}, \str{y} \right\} \in \binom{\C}{2}} \hdist{x_i, y_i}$ as the number of 1s times the number of 0s in the $i$-th column of the matrix: we add $1$ to the summation each time two strings $\str{x}$ and $\str{y}$ differ on column $i$.
	Thus, calling $M_i$ the number of 1s in column $i$, we have
	\begin{align*}
		\frac{2}{M (M-1)}
		\sum_{i = 1}^n
		\sum_{\left\{ \str{x}, \str{y} \right\} \in \binom{\C}{2}} \hdist{x_i, y_i}
		& =
		\frac{2}{M (M-1)}
		\sum_{i = 1}^{n}
		M_i \cdot \left( M - M_i \right)
		\\
		& \le
		\frac{2n}{M (M-1)} \left( \frac{M}{2} \right)^2
		\\
		& =
		\frac{n M}{2 (M-1)}.
	\end{align*}
	So the minimum distance $d$ is
	\begin{equation} \label{eq:plotkin-bound}
		d = \hdist{\C} \le \frac{n M}{2 (M-1)}.
	\end{equation}

	To complete the proof we just need some manipulation of \cref{eq:plotkin-bound}
	\begin{align*}
		2 (M - 1) d & \le n M
		\\
		\implies &
		\\
		2 M d - 2 d & \le n M
		\\
		\implies &
		\\
		M (2 d - n) & \le 2 d
		\\
		\implies &
		\\
		M (2 n \delta - n) & \le 2 n \delta
		\tag{since $d = n \delta$}
		\\
		\implies &
		\\
		M (2 \delta - 1) & \le 2 \delta
		\tag{if $\delta > \frac{1}{2}$ we can divide}
		\\
		\implies &
		\\
		M & \le \frac{2 \delta}{2 \delta - 1}.
	\end{align*}
	So, when $\delta \ge \frac{1}{2}$, we have that $M(n, n \delta)$ is a constant.
	Thus $R(\delta) = 0$, since it is defined as the limit superior of $\frac{1}{n} M(n, n \delta)$.
\end{proof}

\section{Parity Check Codes}

Consider the set
\begin{equation*}
	\C_n
	=
	\left\{
		\str{x} : \str{x} \in \{0, 1\}^n, \,
		2 | \sum_{i=1}^n x_i
	\right\},
\end{equation*}
that is, the set of binary strings which have an even number of 1s.
$\abs{\C_n} = 2^{n-1}$.
This set will not help us correct errors, but it will detect a single error (or in fact an odd number of errors).
Consider $\{0, 1\}^n$ as a $n$-dimensional vector space over $GF(2)$, the Galois field over $\{0,1\}$.

Define $\DotProduct{\str{x}}{\str{y}}$ to be the scalar product between $\str{x}$ and $\str{y}$, \ie
\begin{equation*}
	\DotProduct{\str{x}}{\str{y}} = \sum_{i = 1}^n x_i \cdot y_i \mod 2.
\end{equation*}
$\C_n$ can be defined also as
\begin{equation*}
	\C_n = \left\{ \str{x} : \DotProduct{\str{x}}{\one} = 0 \right\}.
\end{equation*}

It is a hyperplane made of vectors orthogonal to $\one$.
This can be made more general, and fix an arbitrary vector $\str{s}$,
\begin{equation*}
	\C_n(\str{s}) = \left\{ \str{x} : \DotProduct{\str{x}}{\str{s}} = 0 \right\}.
\end{equation*}

Consider the set $S \subseteq [n]$ of indices of $\str{s}$ which are 1.
If we take $\C_n(\str{s})$, we are only looking for errors on $x_i$ for $i \in S$.
$\C_n(\str{s})$ is a linearly closed space, with addition $\xor \mod 2$.
Furthermore, these spaces are closed under intersection.

We can take a bunch of vectors, and the hyperplanes orthogonal to them, and their intersection
\begin{equation*}
	\bigcap_{\str{s} \in S} \C_n(\str{s})
\end{equation*}
In this way we can construct codes that not only detect errors, but also correct them.

A set $\L \subseteq \{0,1\}^n$ is a linear space if
\begin{itemize}
	\item $\L \neq \emptyset$;
	\item is closed under linear combination, \ie $\forall \str{x}, \str{y} \in \L$, $\str{x} \xor \str{y} \in \L$.
\end{itemize}

Take a linear space and consider its orthogonal complement
\begin{equation*}
	\L^\perp = \left\{
		\str{z} : \str{z} \in \{0, 1\}^n, \forall \str{x} \in \L . \DotProduct{\str{z}}{\str{x}} = 0
	\right\}
	=
	\bigcap_{\str{x} \in \L} \C_n(\str{x}).
\end{equation*}

Consider a basis of $\L^\perp$.
Write the vectors from this basis as column vectors of a matrix $A$ with $n$ rows and $m$ columns.

For all $\str{x} \in \L$, we have that $\str{x} \times A = \zero_m$.
So $\L = \left\{ \str{x} : \str{x} \times A = \zero_m \right\}$.
A linear code can be specified by the matrix $A$, which is called the parity check matrix of $\L$.
Given a matrix $A$ we have that
\begin{align*}
	\ker A & = \left\{ \str{x} : \str{x} \times A = \zero_m \right\},
	\\
	\Im A & = \left\{ \str{z} : \str{z} \in \{0, 1\}^m, \exists \str{x} \in \{0, 1\}^n . \str{x} \times A = \str{z} \right\}.
\end{align*}
So the set $\Im A$ is the linear combination of the rows of $A$.
Consider the $i$-th canonical vector $e_i \in \{0, 1\}^n$, we have that
\begin{equation*}
	e_i \times A = A_i^T
\end{equation*}
that is, the $i$-th row of $A$.

Remind that given a code $\C$, if $\hdist{\C} = d$ then we can correct up to $\frac{d-1}{2}$ errors.
Also, recall the definition of Hamming weight:
\begin{equation*}
	\hweight{\str{x}} = \sum_{i = 1}^n x_i.
\end{equation*}

\begin{obs}[Hamming distance of a linear code]
	If $\L \subseteq \{0, 1\}^n$ is a linear code, then
	\begin{equation*}
		\hdist{\L} = \min_{\str{x} \in \L : \str{x} \neq \zero} \hweight{\str{x}}.
	\end{equation*}
\end{obs}

\begin{proof}
	Consider $\str{x} \in \L$ different from $\zero$ and such $\str{x}$ minimises $\hweight{\str{x}}$.
	Then
	\begin{equation*}
		\hdist{\L} \le \hdist{\zero, \str{x}} = \hweight{\str{x}} = \min_{\str{x} \in \L : \str{x} \neq \zero} \hweight{\str{x}}.
	\end{equation*}

	Now we prove that
	\begin{equation*}
		\hdist{\L} \ge \min_{\str{x} \in \L : \str{x} \neq \zero} \hweight{\str{x}}.
	\end{equation*}
	To prove this we rely on the intuitive idea that distance is translation invariant.
	Note that for any $\str{z} \in \L$ we have that
	\begin{equation*}
		\str{z} \xor \L
		=
		\left\{ \str{z} \xor \str{x} : \str{x} \in \L \right\}
		=
		\L,
	\end{equation*}
	since $\L$ is linearly closed.

	Consider any $\str{x}$ and $\str{y}$.
	Since $\str{x} \xor \str{y} \in \L$, we have that
	\begin{equation*}
		\hdist{\str{x}, \str{y}}
		=
		\hdist{\zero, \str{x} \xor \str{y}}
		=
		\hweight{\str{x} \xor \str{y}}
		\ge
		\min_{\str{z} \in \L : \str{z} \neq \zero} \hweight{\str{z}}. \qedhere
	\end{equation*}
\end{proof}

$M(n, 3)$ is the largest cardinality of a code correcting 1 error.
In the proof of the Hamming bound (\cref{thm:hamming-bound}) we have seen that we can build such a code using disjoint balls of radius 1.
\begin{equation*}
	2^n
	\ge
	\abs{\bigcup_{\str{x} \in \C} \hball{\str{x}}{1}}
	=
	\sum_{\str{x} \in \C} \abs{\hball{\str{x}}{1}}
	=
	\abs{\C} \cdot (n + 1)
	\implies
	\abs{\C} \le \frac{2^n}{n+1},
\end{equation*}
with equality if the Hamming Balls cover the entire space.
So we have that
\begin{equation*}
	M(n, 3) \le \frac{2^n}{n+1}.
\end{equation*}

\begin{thm}[Hamming theorem on \aclp{ECC} for 1 error]
	If $\exists m$ such that $n = 2^m - 1$, then
	\begin{equation*}
		M(n, 3) = \frac{2^n}{n+1}.
	\end{equation*}
\end{thm}

\begin{proof}
	Consider all the non-zero vectors in $\{0, 1\}^m$.
	Let $A$ be a matrix having all these strings as its rows ($2^m-1 = n$ rows, $m$ columns).
	The code we are looking for is $\C = \ker A$.

	We have to prove that $\hdist{\C} \ge 3$, and that consequently
	\begin{equation*}
		\min_{\str{x} \in \C : \str{x} \neq \zero} \hweight{\str{x}} = 3,
	\end{equation*}
	or equivalently that $\forall \str{x} \in \C : \str{x} \neq \zero$, $\hweight{\str{x}} \geq 3$.
	\begin{itemize}
		\item If $\str{x} \in \C$ and $\str{x} \neq \zero$, then $\hweight{\str{x}} \neq 1$.
			This is true since if $\hweight{\str{x}} = 1$ then $\str{x} = e_i$ for some $i$, and $e_i \times A$ is the $i$-th row of $A$, which is different from $\zero_m$;
		\item if $\hweight{\str{x}} = 2$ then $\str{x} \not\in \C$.
			If $\hweight{\str{x}} = 2$ then $\str{x} = e_i \xor e_j$ for two distinct $i, j$.
			But then $\str{x} \times A$ is a linear combination of two rows of $A$, and since all rows of $A$ are different $e_i \times A \xor e_j \times A \neq \zero_m$.
	\end{itemize}

	What is left to show is that the Hamming Balls of radius 1 around the elements of $\C$ fill up $\{0, 1\}^n$, \ie
	\begin{equation*}
		\forall \str{z} \in \{0, 1\}^n .
		\exists \str{x} \in \C :
		\str{z} \in \hball{\str{x}}{1}.
	\end{equation*}

	\begin{itemize}
		\item If $\str{z} \in \C$, we are ok;
		\item if $\str{z} \not\in \C$, then $\str{z} \times A \neq \zero_m$.
			We have that $\str{z} \times A \in \{0, 1\}^m$, and since $\str{z} \times A \neq \zero_m$, it must be that it is a row of $A$.
			It follows that $\str{z} \times A = e_i \times A$ for some $i$.
			So $(\str{z} \xor e_i) \times A = \zero_m$, thus $\str{z} \xor e_i \str{x} \in \ker A$.
			So we have found that $\str{z} \in \hball{\str{x}}{1}$. \qedhere
	\end{itemize}
\end{proof}
